# -*- org-emphasis-alist: (("*" bold) ("/" italic) ("_" underline) ("=" org-verbatim verbatim) ("~" org-code verbatim) ("+" (:strike-through nil))); -*-
#+BEGIN_OPTIONS
#+TITLE: Lecture Notes for CBE 60547
#+AUTHOR: William F. Schneider
#+EMAIL: wschneider@nd.edu
#+ADDRESS: University of Notre Dame
#+LATEX_CLASS_OPTIONS: [11pt]
#+LATEX_HEADER:\usepackage[left=1in, right=1in, top=1in, bottom=1in, nohead]{geometry}
#+LATEX_HEADER:\geometry{letterpaper}
#+LATEX_HEADER:\usepackage{outline}
#+LATEX_HEADER:\usepackage{amsmath}
#+LATEX_HEADER:\usepackage{graphicx}
#+LATEX_HEADER:\usepackage{epstopdf}
#+LATEX_HEADER:\usepackage{siunitx}
#+LATEX_HEADER:\usepackage{bm}
#+LATEX_HEADER:\usepackage{framed,color}
#+LATEX_HEADER:\definecolor{shadecolor}{rgb}{1.0,0.8,0.3}
#+LATEX_HEADER:\usepackage{parskip}
#+LATEX_HEADER:\usepackage[version=3]{mhchem}
#+LATEX_HEADER:\usepackage[labelfont=bf]{caption}
#+LATEX_HEADER:\usepackage{hyperref}
#+LATEX_HEADER:\hypersetup{colorlinks=true,urlcolor=blue}
#+LATEX_HEADER:\setlength{\headheight}{15.2pt}
#+LATEX_HEADER:\usepackage{fancyhdr}
#+LATEX_HEADER:\pagestyle{fancy}
#+LATEX_HEADER:\fancyhf{}
#+LATEX_HEADER:\renewcommand{\headrulewidth}{0.5pt}
#+LATEX_HEADER:\renewcommand{\footrulewidth}{0.5pt}
#+LATEX_HEADER:\lfoot{\today}
#+LATEX_HEADER:\cfoot{\copyright\ 2019 W.\ F.\ Schneider}
#+LATEX_HEADER:\rfoot{\thepage}
#+LATEX_HEADER:\lhead{\em{Computational Chemistry}}
#+LATEX_HEADER:\rhead{ND CBE 60547}
#+LATEX_HEADER:\def\dbar{{\mathchar'26\mkern-12mu d}}

#+OPTIONS: toc:1
#+OPTIONS: H:3 num:3
#+OPTIONS: ':t
#+END_OPTIONS

* COMMENT Course agenda

** Desk reserve

*** Cramer book

*** DFT book

** TODO CRC accounts for all

** TODO afs access for all

** Laboratories

*** Lab 1: Intro to computing and fda code (Jan 22)

**** Before HW 2 (Jan 20?)

*** Lab 2: Intro to Avogadro and GAMESS calculations (Feb 5)

**** Before HW 3 (

*** Lab 3: Intro to ASE and Vasp (Feb 26)

*** Lab 4: Intro to surface calculations (Mar )
** Homework

*** Homework 1 (Jan 20)

**** The context of Computational Chemistry

*** Homework 2 (Jan 27)

**** QM review

**** FDA calculations

*** Homework 3 (Feb 12)

**** GAMESS calculations

*** Homework 4 (Feb 26)
**** PES calculation
*** Project 1 (Mar 5)
*** Homework 5 (Mar 17)
**** First Vasp calculations

*** Homework 6 (Mar 31)
**** Second Vasp calculations
*** Project 2 (Apr 7)
*** Project Final (Apr 28)
* Introduction
** What do we care about?

Things chemistry/materials-related:

- What are the properties of atoms?
- What molecules do they make?  What other substances do they make?
- What are the shapes of those molecules?  Structures of those solids?  Properties of them?
- How do those substances react with each other?
- What are the energies of those reactions?
- What are the rates of those reactions?
- What is the strongest substance?
- How do we make a substance to do....?
- add your own questions....

Things that relate to the /chemical/ properties of substances.

** How are we going to figure these out?  With only a computer?
- 1926 :: Erwin Schr\ouml{}dinger equation: $\hat{H}\Psi=E\Psi$
- 1929 :: Paul Dirac, British physicist

#+BEGIN_QUOTE
  The fundamental laws necessary for the mathematical treatment of a
  large part of physics and the whole of chemistry are thus /completely
  known/, and the difficulty lies only in the fact that application of
  these laws leads to equations that are /too complex to be solved/.

  It therefore becomes desirable that /approximate practical methods/ of
  applying quantum mechanics should be developed, which can lead to an
  explanation of the main features of complex atomic systems without
  too much computation.
#+END_QUOTE

- 1930's-1950's :: Elaboration, analytical applications

- 1950's :: Computers start to appear for technical applications

- 1960's :: Density functional theory emerges.

- 1960's-1970's :: Numerical solutions of Schr\ouml{}dinger equation for atoms/molecules---expert users

- 1980's :: "Supercomputer" era---routine use of computational chemistry software becomes possible
#+CAPTION: Ohio State Cray Y-MP supercomputer, ca. 1989.  World's fastest computer at the time.  333 MFlop top speed, 512 Mb RAM
#+ATTR_LATEX: :width 0.2\textwidth
[[./Images/CrayYMPb.jpg]]

- 1990's :: "Chemical accuracy" era---very precise solutions routinely available, for a cost!  See [[./Resources/1994_WFS_JPC.pdf][Schneider, /JPC/ *1994*]].

- 1990's :: Density functional theory (DFT) allows applications to solids/surfaces/liquids to become common. See [[./Resources/1998_Hass_Science.pdf][Hass, /Science/, *1998*]]

- 1990's :: Visualization moves to the desktop

- 2000's :: Computational "screening," materials discovery ([[http://www.crc.nd.edu/~wschnei1/courses/CBE_547/Resources/2010_Gurkan_JPCL.pdf][Gurkan, /J. Phys. Chem. Lett./, *2010*]]), materials genome ([[https://materialsproject.org/]]).

- Today :: Computational chemistry widely integrated into all aspects of chemical, materials, biological research

=Computational chemistry= is now so vast it is impossible to cover everything completely.  We limit ourselves to quantum-mechanics-based calculations.

** Our goals
1. Understand when it is appropriate to use quantum-mechanics methods.
2. Be able to state the basic theoretical, mathematical, and numerical concepts behind quantum mechanical "wavefunction theory" (WFT) and "density functional theory," (DFT) calculations.
3. Understand the terminology and practical issues associated with doing quantum chemical simulations.
4. Get hands-on experience with these concepts using popular computational tools of today, including GAMESS for molecular systems and Vasp for condensed phase systems.
5. Learn how to set up, execute, and analyze results in a modern, Python notebook environment.
6. Learn how to apply the results of quantum chemical simulations to calculate things you care about.
7. Demonstrate an ability to formulate a problem and apply QM  methods to it.
8. Develop the skills to understand a literature paper in the area.

** Reading resources
- These notes
- Chris Cramer, /Essentials of Computational Chemistry/, Wiley, 2004
- Martin, /Electronic Structure/, Cambridge, 2004
- Sholl and Steckel, /Density Functional Theory: A Practical Introduction/, Wiley, 2009
- Kitchin book, [[http://kitchingroup.cheme.cmu.edu/dft-book/]]

** Software tools
*** Notebooks
- [[https://jupyter.org/][jupyter]]/ipython
- [[https://www.gnu.org/software/emacs/][emacs]]/org-mode

*** Molecular methods
- Avogadro environment [[http://avogadro.cc/wiki/Main_Page]]
- GAMESS code [[http://www.msg.ameslab.gov/GAMESS/GAMESS.html]]

*** Supercell methods
- Vasp code [[http://www.vasp.at/]]
- ASE environment [[https://wiki.fysik.dtu.dk/ase/]]

*** Great for getting started
- Webmo [[http://www.webmo.net/]]
\newpage

* Refresher on Quantum Mechanics
** Why quantum mechanics?
Want to describe "mechanics" (equations of motion) of atomic-scale things, like electrons in atoms and molecules

Why? These ultimately determine the energy, the shape, and all the properties of matter.

/de Broglie wavelength/ (1924)
\begin{equation}
\lambda  = h/p = h/mv
\end{equation}
\begin{equation}
h  = \SI{6.626e-34}{J.s} \text{(Planck's constant)}
\end{equation}

|--------------------+----------------------------------+--------------------------------|
|                    | Car                              | Electron                       |
|--------------------+----------------------------------+--------------------------------|
| mass $m$           | \SI{1000}{kg}                    | \SI{9.1e-31}{kg}               |
| velocity $v$       | \SI{100}{km/hr}                  | \SI{0.01}{c}                   |
|                    | typical value on the highway     | typical value in an atom       |
| momentum $p$       | \SI{2.8e-4}{kg.m/s}              | \SI{2.7e-24}{kg.m/s}           |
| wavelength \lambda | \SI{2.4e-38}{m}                  | \SI{2.4e-10}{m}                |
|                    | too small to detect.  Classical! | Comparable to size of an atom. |
|                    |                                  | /Must/ treat with QM!          |
|--------------------+----------------------------------+--------------------------------|

How to describe wave properties of an electron?  Schr\ouml{}dinger equation (1926)

#+BEGIN_CENTER
Kinetic energy + Potential energy = Total Energy
#+END_CENTER

Expressed as differential equation (Single particle, non-relativistic):
\begin{equation}
-\frac{\hbar^2}{2m}\nabla^2 \Psi(\mathbf{r},t) + V(\mathbf{r},t)  \Psi(\mathbf{r},t) = -i \hbar \frac{\partial}{\partial t}  \Psi(\mathbf{r},t)
\end{equation}

If the potential \(V\) is time-invariant, can use separation of variables to
show that the steady-state, time-independent solutions are characterized by an
energy \(E\) and described by:
\begin{eqnarray}
-\frac{\hbar^2}{2m}\nabla^2 \psi(\mathbf{r}) + V(\mathbf{r})  \psi(\mathbf{r}) = E \psi(\mathbf{r}) \\
\Psi(\mathbf{r},t) = \psi(\mathbf{r})e^{-iEt/\hbar}
\end{eqnarray}

** Postulates of non-relativistic quantum mechanics
#+BEGIN_EXPORT latex
\begin{table} 
\begin{center}
    \caption{\large{Postulates of Non-relativistic Quantum Mechanics}}
   \begin{description}
    \item[Postulate 1:] {{\bf The physical state of a system is completely described by
        its wavefunction $\Psi$.}  In general, $\Psi$ is a complex function of the spatial
      coordinates and time.  $\Psi$ is required to be:}
    \begin{outline}
      \item{Single-valued}
      \item {continuous and twice differentiable}
      \item {square-integrable ($\int \Psi^*\Psi d\tau$ is defined over all finite domains)}
      \item {For bound systems, $\Psi$ can always be normalized such that $\int \Psi^*\Psi d\tau=1$}
    \end{outline}

  \item[Postulate 2:]  To every physical observable quantity $M$ there corresponds a
    Hermitian operator $\hat{M}$.  {\bf The only observable values of $M$ are the
      eignevalues of $\hat{M}$.}
    \begin{center}
    \begin{tabular}[h]{ccc}
      \hline
{\bf Physical quantity} & {\bf Operator} & {\bf Expression} \\
\hline
Position $x,y,z$ & $\hat{x},\hat{y},\hat{z}$ & $x\cdot, y\cdot, z\cdot$ \\ \\
Linear momentum $p_x, \ldots$ & $\hat{p}_x,\ldots $ & $\displaystyle -i\hbar\frac{\partial}{\partial
  x},\ldots $\\
Angular momentum $l_x, \ldots$ & $\hat{p}_x,\ldots $ & $\displaystyle -i\hbar \left
  (y\frac{\partial}{\partial z}-z\frac{\partial}{\partial y}\right ), \ldots $ \\
Kinetic energy $T$ & $\hat{T}$ & $\displaystyle -\frac{\hbar^2}{2m}\nabla^2$ \\
Potential energy $V$ & $\hat{V}$ & $V({\bf r},t)$ \\
Total energy $E$ & $\hat{H}$ & $\displaystyle -\frac{\hbar^2}{2m}\nabla^2+V({\bf r},t)$\\ \\
\hline
    \end{tabular}
  \end{center}
    \item[Postulate 3:] {If a particular observable $M$ is measured many times on many
      identical systems is a state $\Psi$, the average resuts with be the expectation
      value of the operator $\hat{M}$:
      \begin{equation*}
        \langle M \rangle = \int \Psi^* (\hat{M}\Psi)d{\bf\tau}
      \end{equation*}}
    \item[Postulate 4:] {The energy-invariant states of a system are solutions of the equation
        \begin{eqnarray*}
          \hat{H}\Psi({\bf r},t) & = & i\hbar\frac{\partial}{\partial t}\Psi({\bf r},t) \\
          \hat{H} & = & \hat{T}+\hat{V}
        \end{eqnarray*}
      The time-independent, stationary states of the system are solutions to the equation
      \begin{equation*}
        \hat{H}\Psi({\bf r}) = E\Psi(\bf{r})
      \end{equation*}
}
    \item[Postulate 5:] (The {\bf uncertainty principle}.)  Operators that do not commute
      $(\hat{A}(\hat{B}\Psi)\neq\hat{B}(\hat{A}\Psi))$ are called {\em conjugate}.
      Conjugate observables cannot be determined simultaneously to arbitrary accuracy.
      For example, the standard deviation in the measured positions and momenta of
      particles all described by the same $\Psi$ must satisfy $\Delta x\Delta p_x \geq \hbar/2$.
    \end{description}
\end{center}
\end{table}
#+END_EXPORT

** Notes on constants and units
Resource on physical constants: [[http://physics.nist.gov/cuu/Constants/]]
Resource for unit conversions: [[http://www.digitaldutch.com/unitconverter/]]

Unit converter available in Calc mode in [[https://www.gnu.org/software/emacs/][Gnu emacs]] *highly recommended*

#+CAPTION: Atomic units common for quantum mechanical calculations (see [[http://en.wikipedia.org/wiki/Atomic_units]])
|---------------------+--------------------------+--------------------------------+-----------------------|
|                     | Atomic unit              | SI unit                        | Common unit           |
|---------------------+--------------------------+--------------------------------+-----------------------|
| Charge              | $e = 1$                  | \SI{1.6021e-19}{C}             |                       |
| Length              | $a_0 = 1$ (bohr)         | \SI{5.29177e-11}{m}            | \SI{0.529177}{\AA}    |
| Mass                | $m_e = 1$                | \SI{9.10938e-31}{kg}           |                       |
| Angular momentum    | $\hbar = 1$              | \SI{1.054572e-34}{J.s}         |                       |
| Energy              | $E_h = 1$ (hartree)      | \SI{4.359744e-18}{J}           | \SI{27.2114}{eV}      |
| Electrostatic force | $1/(4\pi\epsilon_0) = 1$ | \SI{8.987552e-9}{N.m^2/C^2}    |                       |
| Boltzmann constant  |                          | \SI{1.38065e-23}{J\per K}      | \SI{8.61733e-5}{eV/K} |
|---------------------+--------------------------+--------------------------------+-----------------------|


#+BEGIN_CENTER
Energy units
1 eV = \SI{1.60218e-19}{J} = \SI{96.485}{kJ/mol} = \SI{8065.5}{cm^{-1}} = \SI{11064}{K.k_B}
#+END_CENTER

** Example: Energy states of a particle in a box      

System defined by potential experienced by particle:

$V(\mathbf{r}) = 0,\qquad 0 < x,y,z < L$

$V(\mathbf{r}) = \infty,\qquad x,y,z \leq 0,\ x,y,z \geq L$

#+BEGIN_CENTER
#+ATTR_LATEX: :width 0.3\textwidth
[[./Images/Cube.png]]
#+END_CENTER

3D box \rightarrow 3 degrees of freedom/coordinates

*Schr\ouml{}dinger equation*
\begin{equation}
-\frac{\hbar^2}{2m_e} \left ( \frac{\partial^2 }{\partial x^2} + \frac{\partial^2 }{\partial y^2} + \frac{\partial^2 }{\partial z^2} \right ) \psi(x,y,z) = E \psi(x,y,z)
\end{equation}
\begin{equation}
\psi(x,y,z) = 0, \quad x,y,z \leq 0,\ x,y,z \geq L
\end{equation}

A second-order, linear, partial differential equation.  Boundary value problem. Solve by separation of variables.  Postulate $\psi(x,y,z) = X(x)Y(y)Z(z)$. Substituting and rearrange to get

\begin{equation}
-\frac{\hbar^2}{2m_e} \left (\frac{1}{X(x)}\frac{\partial^2 X(x)}{\partial x^2} + \frac{1}{Y(y)}\frac{\partial^2 Y(y)}{\partial y^2} + \frac{1}{Z(z)}\frac{\partial^2 Z(z)}{\partial z^2} \right ) = E \qquad 0 < x,y,z <L
\end{equation}

ftn x + ftn y + ftn z = constant \rightarrow each term must be constant.

*Equation for each dimension*
\begin{equation}
-\frac{\hbar^2}{2m_e}\frac{\partial^2 X(x)}{\partial x^2} = E_x X(x), \qquad X(0)=X(L) = 0
\end{equation}

Seek function that twice differentiated returns itself and satisfies boundary conditions.
\begin{equation}
X(x) = \sin\frac{n_x\pi x}{L},\qquad n_x = 1,2,3,\ldots
\end{equation}

\begin{equation}
E_{n_x} = \frac{n_x^2\pi^2\hbar^2}{2 m_e L^2}
\end{equation}

Solutions called /eigenfunctions/ (or /wavefunctions/) and /eigenvalues/.  Characterized
by /quantum numbers/, one for each degree of freedom.  These (and all QM) solutions have
certain special properties, including that they /orthonormal/ and form a /complete set/.

*Normalization*

Seek a constant such that the inner eigenfunction product is unity.
\begin{eqnarray}
C^2 \int_0^L \sin^2 \frac{n_x\pi x}{L} dx = C^2 L/2 = 1 \rightarrow C=\pm\sqrt{\frac{2}{L}}\\
X(x) = \pm\sqrt{\frac{2}{L}}\sin\frac{n_x\pi x}{L},\qquad n_x = 1,2,3,\ldots
\end{eqnarray}

*Orthonormal*
\begin{equation}
\langle X_{n_x} | X_{n^\prime_x} \rangle = \delta_{n_{x},n_x^\prime}\qquad
\text{Dirac notation}
\end{equation}

#+begin_src python :exports results
import numpy as np
import matplotlib.pyplot as plt

hc = 1239.8      # eV nm
c = 2.9979e8 * 1.e9   # nm/s
k = 8.61734e-5   # eV /K
hck = hc/k       # nm K
me = 5.685630e-30  # eV/(nm/s)^2
hbar = 6.58212e-16 # eV s 

L = 1.0 ; # nm
E0 = (1./(2.*me))*(np.pi * hbar/L)**2  # eV

def psi(n,x):
      return np.sqrt(2./L)*np.sin(n*np.pi*x/L)
def eig(n):
      return E0*n*n

plt.figure()
wl=np.linspace(0,L,500)
for n in [1,2,3,4,5]:
    waveftn = psi(n,wl)
    energy = eig(n)
    plt.plot([0,L],[energy,energy],ls='--',color='black')
    offset = waveftn + energy
    plt.plot(wl,offset,label='n = {}'.format(n))

legend=plt.legend()
plt.xlabel('Distance (L)')
plt.ylabel('Energy (eV)')
plt.title('Energies and wavefunctions of an electron confined to a 1 nm box')
plt.savefig('./Images/1DPIAB.png')
#+end_src

#+RESULTS:

#+ATTR_LATEX: :width 0.75\textwidth
[[./Images/1DPIAB.png]]

- Energy increase with number of /nodes/.

- Is this real?  See [[http://dx.doi.org/10.1021/jp053496l][Ho, /J. Phys. Chem. B/ *2005*, /109/, 20657]].  Where /is/ the electron?

#+BEGIN_SRC python :exports results
# Stub to solve Schrodinger eq numerically
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
from scipy import linalg as LA

hc = 1239.8      # eV nm
c = 2.9979e8 * 1.e9   # nm/s
kB = 8.61734e-5   # eV /K
hckB = hc/kB       # nm K
me = 5.685630e-30  # eV/(nm/s)^2
hbar = 6.58212e-16 # eV s 
mH = 1.0364269572e-26 # eV/(nm/s)^2

# harmonic parameters
k = 800.0 *  6.24150912588 # eV / nm^2


def harmonic(k,x):
    return 0.5 * k * x**2

def squarewell(x):
    n = np.size(x)
    x = np.zeros(x-1)
    x[0] = x[n-1] = 10
    return x

def Hmat(mass,V):
    hbar2mu = -hbar*hbar/(2 * mass) # eV nm^2
    n = np.size(V)
#    print(n)
#    print(V)

    A = np.zeros((n-1,n-1))
    for i in range(n-1):
        for j in range(n-1):
            if i==j+1 or i==j-1:
                 A[i,j]=(3./2.)*hbar2mu
            if i==j+2 or i==j-2:
                 A[i,j]=-(3./20.)*hbar2mu
            if i==j+3 or i==j-3:
                 A[i,j]=(1./90.)*hbar2mu
        A[i,i]=-(49./18.)*hbar2mu +V[i] 
    return A

X = np.linspace(-0.01,0.01,10)
# print(X)
# print(np.size(X))
V = harmonic(k,X)
#print(V)
#print(np.size(V))
H = Hmat(mH,V)
#print(H)
eigenE, eigenfn = LA.eigh(H)

# print(eigenE)

#plt.figure()
#plt.plot(X,V)
#plt.xlabel('Distance (nm)')
#plt.ylabel('Energy (eV)')
#plt.show()
# plt.savefig('./Images/harmonic.png')


#+END_SRC

#+RESULTS:

*Complete set*
Any function on the same domain that satisfies the same boundary conditions can be represented as a linear combination of these solutions:
\begin{eqnarray}
f(x) &=& \sum_i \left \{\int_0^L X_i(x) f(x) dx \right \}  X_i(x)  = \sum_i C_i X_i(x)\\
|f\rangle &= &\sum_i |X_i\rangle\langle X_i | f\rangle\quad\text{Dirac notation}
\end{eqnarray}
Illustrates idea of a basis set. These functions are the basis in "plane wave" supercell methods.

*Three-dimensional solutions*
\begin{equation}
\psi(x,y,z) = X(x)Y(y)Z(z) = \left ( \frac{2}{L} \right )^{3/2} \sin\frac{n_x\pi x}{L}\sin\frac{n_y\pi y}{L}\sin\frac{n_z\pi z}{L},\qquad n_{x},n_{y},n_{z}=1,2,3,\ldots
\end{equation}
\begin{equation}
\label{eq:2}
E = E_{x}+E_{y}+E_{z}=\frac{(n_{x}^{2}+n_{y}^{2}+n_{z}^{2}) \pi^{2}\hbar^{2}}{2 m L^{2}}
\end{equation}

#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/2DSine1.png]]
#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/2DSine2.png]]


#+CAPTION: Energy sates of 3D Particle in a box
[[./Images/3DEnergyStates.png]]

Properties of solutions:
- Symmetry of system introduces degeneracy in solutions
- Energy depends on volume \rightarrow pressure!

\newpage

* Hydrogen atom: simplest chemical "thing"
** Schr\ouml{}dinger equation
Place massive nucleus at origin and describe position of electron in spherical coordinates

#+ATTR_LATEX: :width 0.3\textwidth
[[./Images/spherical.png]]

\begin{eqnarray}
\left\{-\frac{\hbar^2}{2m_e}\nabla^2 + V(r)\right\} \psi(\bm{r}) &=& E \psi(\bm{r})\\
V(r) &=& -\frac{e^2}{4\pi\epsilon_0}\frac{1}{|\bm{r}|}
\end{eqnarray}
Coulomb potential---our nemesis!  Decays slowly with distance.  Boundary conditions?
#+BEGIN_EXPORT latex
\begin{table}[]
   \begin{center}
   \caption{Hydrogen atom}
    \label{Hydrogen atom}
\begin{tabular}[h]{|c|}
\hline
 \\
$\displaystyle       V(r) = -\frac{e^2}{4\pi\epsilon_0}\frac{1}{r}, 0 < r< \infty$ \\
 \\
$\displaystyle     \hat H = -\frac{\hbar^2}{2m_e}\left \{ \frac{1}{r^2}\left [
  \frac{\partial}{\partial r}r^2\frac{\partial}{\partial r}  \right ] - \frac{\hat{L}^2}{\hbar^2 r^2} \right \} +V(r)$ \\
\\
$\displaystyle     \hat L^2 = -\hbar^2 \left [
  \frac{1}{\sin^2\theta}\frac{\partial^2}{\partial \phi^2}+\frac{1}{\sin
    \theta}\frac{\partial}{\partial \theta}\left ( \sin \theta
    \frac{\partial}{\partial \theta}\right ) \right ] $ \\
\\
$\displaystyle \psi(r,\theta,\phi) = R(r)Y_{l,m_l}(\theta,\phi) $ \\
\\
$\displaystyle   \left \{ -\frac{\hbar^2}{2m_e}
            \frac{d}{dr^2}  + \frac{\hbar^2
              l(l+1)}{2 m_e r^2}
          -\frac{e^2}{4\pi\epsilon_0}\frac{1}{r}\right \} r R(r) = E r R(r) $ \\
\\
$\displaystyle R_{nl}(r) = N_{nl} e^{-x/2} x^l L_{nl}(x),\ \ \  x = \frac{2 r}{n a_0} $
\\
$\displaystyle P_{nl}(r) = r^2 R_{nl}^2 $
\\
\\
$\displaystyle n = 1, 2, \ldots,\ \  l = 0, \ldots, n-1 \ \ m_l = 0,\pm 1, \ldots, \pm l$
\\
\\
$\displaystyle N_{nl} = \sqrt{\left ( \frac{2}{na_0}\right )^3 \frac{(n-l-1)!}{2n(n+l)!}}$
\\
\\
$\displaystyle L_{10} = L_{21} = L_{32} = \ldots =1 \quad L_{20} = 2 - x \quad L_{31} = 4-x$
\\
\\
\\
$\displaystyle     E_{n}=-\frac{1}{2}\frac{\hbar^2}{m_e a_0^2}\frac{1}{n^2} =-\frac{E_H}{2}\frac{1}{n^2}$ \\
 \\
$\displaystyle |L| = \hbar \sqrt{l(l+1)}, L_z = m_l \hbar $ \\
\\
$\displaystyle \langle r \rangle = \left \{ \frac{3}{2} n^2 - \frac{1}{2} l(l+1) \right \} \frac{a_0}{Z} $ \\
\\
%%     \includegraphics[scale=0.4]{Images/H_atom} \\       
\hline
\end{tabular}
 \end{center}
\end{table}
#+END_EXPORT

** Analytical solutions
1. Separate: $\psi(r,\theta,\phi)=R(r)\Theta(\theta,\phi)$
2. Angular equation \(\hat{L}^2\Theta(\theta,\phi) = E_L\Theta(\theta,\phi)\)
   1. $\Theta=Y_{lm_l}(\theta,\phi)$ are "spherical harmonics", describe angular motion
   2. Azimuthal quantum number $l=0,1,...,n-1$, correspond to $s$, $p$, $d$, \ldots orbital sub-shells; angular "shape," number of angular nodes, angular momentum of electron
   3. Magnetic quantum number $m_l=-l,-l+1,...,l$, \ldots orientation of orbital
3. Radial equation
   \[\left \{ -\frac{\hbar^2}{2m_e}
            \frac{d}{dr^2}  + \frac{\hbar^2
              l(l+1)}{2 m_e r^2}
          -\frac{e^2}{4\pi\epsilon_0}\frac{1}{r}\right \} r R(r) = E r R(r)\]

Solutions are a polynomial * exponential decay.  Exponential part called a /Slater/ function.  Larger the exponent, faster the
      decay.  Degree of polynomial determined by principle quantum number $n=1,2,\ldots$.

Energy expression, corresponds to our conventional H atom spectrum
     \[E_n = -\frac{1}{n^2}\left(\frac{e^2}{2 a_0}\right) = -\SI{13.6}{eV}\cdot\frac{1}{n^2},\qquad n = 1,2,\ldots \]

Questions: Ionization energy of an H atom?  \(1s\rightarrow 2s\) energy?  Thermal populations?

Integrate out angular components to get radial probability function $P_{nl}(r)=r^2 R_{nl}^2(r)$
    \[\langle r\rangle = \int r P_{nl}(r) dr = \left(\frac{3}{2}n^2-l(l+1)\right)a_0\]

Note darn electron doesn't want to stay in the Coulomb well!
   Wavefunction extends beyond the classical region defined by $E_n =
   V(r_\text{classical})$.  This phenomenon is called /tunneling/, is
   a purely quantum mechanical effect, is pervasive in chemistry,
   leading for instance to chemical bonding.


#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np
from sympy.physics.hydrogen import E_nl
from sympy.physics.hydrogen import R_nl
from sympy import var

var("r E")
max = 10

r = np.linspace(0,max,100)
Ones = np.array([R_nl(1, 0, x) for x in r],dtype='float')
E = E_nl(1)
Ones = Ones
plt.plot(r,Ones,label='1s',color='blue')
plt.plot([0,max],[E,E],ls=':',color='blue')

Twos = np.array([R_nl(2, 0, x) for x in r],dtype='float')
E = E_nl(2)
Twos = Twos
plt.plot(r,Twos,label='2s',color='orange')
plt.plot([0,max],[E,E],ls=':',color='orange')
Twop = np.array([R_nl(2, 1, x) for x in r],dtype='float')
Twop = Twop
plt.plot(r,Twop,label='2p')

# plt.plot(r,-1/r+2/r**2,color = 'green', linestyle='--',label='l=1 potential')

plt.plot([0,max],[0,0],color='black',linestyle=':')
plt.plot(r,-1/r,color = 'blue', linestyle='--',label='potential')
plt.xlim(0,max)
plt.ylim(-1,2)
plt.xlabel('Distance (a0)')
plt.ylabel('Wavefunction R(r)')

plt.legend()
plt.title('H atom radial wavefunctions')
plt.savefig('./Images/H-R.png')

plt.figure()
Ones = np.array([x*x*R_nl(1, 0, x)**2 for x in r],dtype='float')
plt.plot(r,Ones,label='1s')
Twos = np.array([x*x*R_nl(2, 0, x)**2 for x in r],dtype='float')
plt.plot(r,Twos,label='2s')
Twop = np.array([x*x*R_nl(2, 1, x)**2 for x in r],dtype='float')
plt.plot(r,Twop,label='2p')

plt.plot([0,max],[0,0],color='black',linestyle=':')
plt.xlim(0,max)
plt.xlabel('Distance (a0)')
plt.ylabel('Radial probability P(r)')
plt.title('H atom radial probability functions')

plt.legend()
plt.savefig('./Images/H-P.png')

#+END_SRC

#+RESULTS:

#+CAPTION: H atom wavefunctions
#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/H-R.png]] 
#+CAPTION: H atom radial probability
#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/H-P.png]] 

#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
from matplotlib import cm, colors
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
from scipy.special import sph_harm

phi = np.linspace(0, np.pi, 100)
theta = np.linspace(0, 2*np.pi, 100)
phi, theta = np.meshgrid(phi, theta)

# The Cartesian coordinates of the unit sphere
x = np.sin(phi) * np.cos(theta)
y = np.sin(phi) * np.sin(theta)
z = np.cos(phi)

m, l = 0, 0

# Calculate the spherical harmonic Y(l,m) and normalize to [0,1]
fcolors = sph_harm(m, l, theta, phi).real
fmax, fmin = fcolors.max(), fcolors.min()
fcolors = (fcolors - fmin)/(fmax - fmin)

# Set the aspect ratio to 1 so our sphere looks spherical
sfig = plt.figure(figsize=plt.figaspect(1.))
s = sfig.add_subplot(111, projection='3d')
s.plot_surface(x, y, z,  rstride=1, cstride=1, facecolors=cm.seismic(fcolors))
# Turn off the axis planes
s.set_axis_off()
plt.savefig('./Images/s.png')

m, l = 0, 1

# Calculate the spherical harmonic Y(l,m) and normalize to [0,1]
fcolors = sph_harm(m, l, theta, phi).real
fmax, fmin = fcolors.max(), fcolors.min()
fcolors = (fcolors - fmin)/(fmax - fmin)

# Set the aspect ratio to 1 so our sphere looks spherical
# fig = plt.figure(figsize=plt.figaspect(1.))
pfig = plt.figure(figsize=plt.figaspect(1.))
p = pfig.add_subplot(111, projection='3d')
p.plot_surface(x, y, z,  rstride=1, cstride=1, facecolors=cm.seismic(fcolors))
# Turn off the axis planes
p.set_axis_off()

plt.savefig('./Images/p.png')

m, l = 1, 2

# Calculate the spherical harmonic Y(l,m) and normalize to [0,1]
fcolors = sph_harm(m, l, theta, phi).real
fmax, fmin = fcolors.max(), fcolors.min()
fcolors = (fcolors - fmin)/(fmax - fmin)

# Set the aspect ratio to 1 so our sphere looks spherical
# fig = plt.figure(figsize=plt.figaspect(1.))
dfig = plt.figure(figsize=plt.figaspect(1.))
d = dfig.add_subplot(111, projection='3d')
d.plot_surface(x, y, z,  rstride=1, cstride=1, facecolors=cm.seismic(fcolors))
# Turn off the axis planes
d.set_axis_off()

plt.savefig('./Images/d.png')
#+END_SRC

#+RESULTS:

#+BEGIN_EXPORT LaTeX
\begin{figure}
\includegraphics[scale=0.4]{./Images/s.png}
\includegraphics[scale=0.4]{./Images/p.png}
\includegraphics[scale=0.4]{./Images/d.png}
\caption{Pythonic $s$ ($l = 0$), $p$ ($l=1$), and $d$ ($l=2$) spherical harmonics. Color scale from red to white to blue corresponds to positive to zero to negative sign of wavefunction.}
\end{figure}
#+END_EXPORT

** /Variational principle/
What if we don't know where to look to find the \(R(r)\)?  Or an analytical solution doesn't exist?  Solve numerically.

\(l=0\) case, in atomic units:

\[\left\{-\frac{1}{2}\frac{d^2}{dr^2} -\frac{1}{r}\right\} rR(r) = ErR(r),\quad 0<r\infty \]

Guess something.  Must obey appropriate boundary conditions and be a well-behaved function.  For example, a /Gaussian/:
\[g_\xi(r) = e^{-\xi r^2}\]
Let's normalize:
\[N = \left\{ \int_0^\infty g_\xi^2(r) r^2 dr \right\}^{-1/2} = 2 \left(\frac{8\xi^3}{\pi}\right)^{1/4} \]

\[\tilde{g}_\xi(r) = N g_\xi(r)\]
Now evaluate energy, say for \(\xi=1\):
\[\langle E \rangle = \langle \tilde{g}_1|\hat{H}|\tilde{g}_1\rangle = \SI{-0.096}{Hartree} \]
Hmmm, not very good, much higher in energy than true answer of \SI{-0.5}{Hartree}. 

Let's try adding two Gaussians together, with equal weight:
\[b(r) = N_s \left(\tilde{g}_1(r) + \tilde{g}_{0.5}(r) \right )\]
Normalize:
\begin{eqnarray}
\langle b(r)|b(r)\rangle & = & N_s^2 \left(\langle\tilde{g}_1 | \tilde{g}_1\rangle + \langle\tilde{g}_{0.5} | \tilde{g}_{0.5}\rangle + 2\langle\tilde{g}_1 | \tilde{g}_{0.5}\rangle\right)\\
 &=& N_s(1+1 + 2 S) =1 \\
N_s &= &\frac{1}{\sqrt{2(1+S)}}
\end{eqnarray}

Note appearance of "overlap integral" \(S=\langle \tilde{g}_1|\tilde{g}_{0.5}\rangle\), shows how similar or different \(g_i\) are.

Re-evaluate energy
\[\langle b(r) |\hat{H} |b(r) \rangle = \SI{-0.306}{Hartree} \]
Much closer to the truth!  

Could even weight the two Gaussians differently:
\[c(r) = N_s^\prime \left(\tilde{g}_1(r) + 1.5 \tilde{g}_{0.5}(r) \right )\]
\[\langle c(r) |\hat{H} |c(r) \rangle = \SI{-0.333}{Hartree} \]
Better yet!

#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np
from scipy.integrate import quad

def Gaussian(xi,x):
    return (np.exp(-xi*x*x))

def integrand(x):
    return (x*x*np.exp(-2.0*x*x))

def Gnorm(xi,r):
    return 2.0*((8.0 * xi**3/np.pi)**0.25)*np.exp(-xi*r*r)

# Show normalization of a Gaussian:
ans, err = quad(integrand, 0, 10)
Norm = 1.0/np.sqrt(ans)
# print(Norm,2*(8.0/np.pi)**0.25)

max = 4
r = np.linspace(0,max,100)

R_10 = 2*np.exp(-r)  # 1s
G1 = Gnorm(1.0,r)    # Gaussian x= = 1

plt.xlim(0,max)
plt.plot(r,R_10,label='1s')
plt.plot(r,G1,label='xi = 1')

Contract1 = Gnorm(1.0,r) + Gnorm(0.5,r)
Overlap = np.trapz(Gnorm(1.0,r)*Gnorm(0.5,r)*r*r,r)
#print(Overlap)
Contract1 = Contract1 * 1.0/np.sqrt(2.0*(1.0 + Overlap))
plt.plot(r,Contract1,label='xi=1 + xi=0.5')

c1 = 1.0; c2 = 1.5
Contract2 = c1* Gnorm(1.0,r) + c2 * Gnorm(0.5,r)
Overlap = np.trapz(Gnorm(1.0,r)*Gnorm(0.5,r)*r*r,r)

Contract2 = Contract2 * 1.0/np.sqrt(c1**2 + c2**2 + 2.0*c1*c2*Overlap)
plt.plot(r,Contract2,label='xi=1 + 1.5 xi=0.5')

plt.legend()
plt.xlabel('Distance (a0)')
plt.ylabel('Wavefunction R(r)')

plt.title('Gaussian H atom radial wavefunctions')
plt.savefig('./Images/Hbasis.png')

# Add code to evaluate energies
#+END_SRC

#+RESULTS:
#+CAPTION: Comparison of exact and approximations 2 H 1s radial function
#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/Hbasis.png]]

Could continue to add Gaussians of various exponents, and could vary weights, or could
even add in any other functions that we want that are "well-behaved." Would find that no
matter what we do, the "model" energy would be greater than the "true" value.  Basis of the /variational principle/:

#+BEGIN_QUOTE
For any system described by the Hamiltonian \(\hat{H}\) and any satisfactory trial wavefunction \(\Psi\),
\[ E = \frac{\langle \Psi | \hat{H} | \Psi \rangle}{\langle \Psi | \rangle } \ge E_0\]
where $E_0$ is the true ground state energy of the system.
#+END_QUOTE

Consequence of the completeness of the solutions of the Schr\ouml{}dinger equation.  Extremely important to us, because we can use the /calculus of variations/ to seek energy-optimal \(\Psi\).

** Basis functions
Recognize that we are approximating

\[R_{10}(r) \approx f(r) = \sum_i c_i \phi_i(r)\]

\(\phi_i\) are /basis functions/ and \(c_i\) are /variational parameters/, or /coefficients/.

If we find \(c_i\) that minimize energy, then we have an optimal approximation to \(E_0\) within our basis, and we are sure that \(E_0\) is an upper bound on the truth.  Adding more basis functions /must/lower energy.

Common trade-offs:
1. Want to choose \(\phi_i\) that are good approximation to the "truth"
2. Want to choose \(\phi_i\) that are mathematically convenient

| Slaters   | Gaussians             | Plane waves       | Mixed basis     |
|-----------+-----------------------+-------------------+-----------------|
| Accurate  | Moderate accuracy     | Poor accuracy     |                 |
| Expensive | modest cost           | cheap!            |                 |
| =ADF=     | =Gaussian=, =GAMESS=, | =Vasp=, =CPMD=,   | =FLAPW=, =CP2k= |
|           | =NWChem=, =Qchem=     | =QuantumEspresso= |                 |

Virtually /all/ quantum codes work on this principle, and the main
differences are in details of implementation and ancillary
functionality provided.

There are exceptions. /GPAW/ for instance solves the QM equations by finite difference expressions on a numerical grid.  Lends itself to parallelization and may be the future\ldots remains to be seen!
** Secular equations
Apply variational principle to two basis functions for the H atom:

\[f(r) =c_1 \phi_1(r) + c_2\phi_2(r) \]

\[\langle E \rangle = \frac{\langle f(r)|\hat{H}|f(r)\rangle}{\langle f(r)|f(r)\rangle} \]

Substitute and solve \(\partial \langle E \rangle/\partial c_1 = \partial \langle E \rangle/\partial c_2 = 0\).  Each gives a linear /secular equation/ in $c_1$ and $c_2$:

\[
\left(\begin{array}{cc}
H_{11} - E & H_{12}-S_{12}E \\
H_{12}-S_{12}E & H_{22} 
\end{array}\right)\left(
\begin{array}{c} c_1 \\c_2\end{array}\right) =0
\]
where \(H_{ij} = \langle\phi_i|\hat{H}|\phi_j\rangle\) is a /matrix element/ and \(S_{ij}=\langle \phi_i | \rangle \phi_j \rangle\) is an /overlap/.  If \(S_{ij} = 0\), basis is /orthogonal/, problem simplifies.  If \(S_{ij}\approx 1\), basis is /redundant/, not efficient!

Evaluate /secular determinant/ 
\[ \left| \begin{array}{cc} H_{11} - E & H_{12}-S_{12}E
\\ H_{12}-S_{12}E & H_{22} \end{array}\right| =0 \]
Gives a quadratic in /E/.  Yields two solutions, which would be approximations to the /1s/ /and/ /2s/ orbital energies of hydrogen. Back substitution to get coeffients:

| 1s: $\langle E_{1s}\rangle > E_{1s,\mathrm{true}}$ |  $c^{1s}_1$ and $c^{1s}_2$ |
| 2s: $\langle E_{2s}\rangle > E_{2s,\mathrm{true}}$ |  $c^{2s}_1$ and $c^{2s}_2$ |

Note we always get one solution for each basis function.  Secular
matrix grows as the square of the number of basis functions, gets
expensive to find roots. 

If basis is not orthogonal, common to /orthogonalize/.  Find linear transformation that makes $\langle S_{ij} = \delta_{ij}$.  Evaluate
\[ \bm{c}^\prime = \bm{S}^{1/2} \bm{c} \rightarrow \hat{\bm{H}}^\prime = \bm{S}^{-1/2}\bm{H}\bm{S}^{1/2}\]

\[
\left(\begin{array}{cc}
H^\prime_{11} - E & H^\prime_{12}\\
H^\prime_{12} & H^\prime_{22} 
\end{array}\right)\left(
\begin{array}{c} c^\prime_1 \\c^\prime_2\end{array}\right) =0
\]

\[\bm{H}^\prime\bm{c}^\prime = E \bm{c}^\prime \]

Secular equations reduce to standard linear eigenvalue problem.  All the tricks of linear
algebra can be applied to find the /eigenvalues/ (orbital energies) and /eigenvectors/
(wavefunctions).  Called /diagonalizing/ the matrix.  Tricks can be used to find the
lowest energy roots only.

Same basic idea is used in virtually all calculations on atoms, molecules, \ldots.  Basis
of /semi-empirical/ and /first principles/ methods.

** Spin
Can't leave the H atom without mentioning electron /spin/.  Non-relativistic QM gives us three quantum numbers. Relativity teaches that space and time are equivalent. Relativistic H atom solutions introduce a 4th degree of freedom that, under many circumstances, decouples from other three.  Call it the electron /spin/, because it behaves like the electron has an intrinsic quantum angular momentum with magnitude \(s = 1/2\).

\[m_s = +1/2, \quad\text{``spin up''},\quad\alpha\]
\[m_s = -1/2, \quad\text{``spin down''},\quad\beta\]

$m_s$ specifies $z$ component of angular momentum, \(s_z = m_s\hbar\).

To fully specify state of H atom, must specify all four quantum numbers.
\newpage

* (Two is too) many electrons
Helium: next (after hydrogen) simplest atom

In a sense, we "know" the answer\ldots $1s^2$.  But is this same $1s$ as H?  No!  Different nuclear charge, interactions between the two electrons.  This is an approximation and a very convenient shorthand!

*** Schr\ouml{}dinger equation for He
Wavefunction \(\Psi(\mathbf{r}_1,\mathbf{r}_2)\), atom energy \(E\).

Define 1-electron operator for each electron, in atomic units.  Include kinetic energy of electron and its attraction to nucleus of charge $Z=2$:
\[\hat{h}_i = -\frac{1}{2}\nabla^2_i -\frac{Z}{|\mathbf{r}_i|}\]
Looks similar to hydrogen atom.

BUT, electrons also repel.  Total Schr\ouml{}dinger equation for He:
\[\left\{\hat{h}_1 + \hat{h}_2 + \frac{1}{|\mathbf{r}_2 - \mathbf{r}_1|\right\}\Psi(\mathbf{r}_1,\mathbf{r}_2) = E\Psi(\mathbf{r}_1,\mathbf{r}_2) }\]
Last term accounts for electron-electron electrostatic repulsion.  Makes problem non-separable and really hard to solve. (How many solutions are there?)

Generalize to \(n\)-electron atom, in atomic units:
\begin{eqnarray}
\hat{H} & = & \sum_i \hat{h}_i + \sum_{j>i+1}\frac{1}{|\mathbf{r}_j - \mathbf{r}_i|} \\
\hat{H} \Psi(\mathbf{r}_1,\ldots,\mathbf{r}_n) & = & E\Psi(\mathbf{r}_1,\ldots,\mathbf{r}_n) \end{eqnarray}

First summation over all electrons, second gets all electron pairs.

Solutions are many-dimensional functions of the coordinates of all the electrons.  Cannot solve this analytically, although approaches exist (eg quantum Monte Carlo) that can in principle get very close.  Thankfully, though, we can make approximations that work out really well.  We'll look at three historically important ones.

** The Hartree atom
Simplest approach is to approximate \(\Psi\). Douglas Hartree (1897-1958) writes:
\[ \Psi(\mathbf{r}_1,\mathbf{r}_2) \approx \psi_1(\mathbf{r}_1)\cdot \psi_2(\mathbf{r}_2)\]
So-called Hartree product.  Can't be right.  It gives the probability of two electrons being in the same place as some number $> 0$!  Neglects /electron correlation/.  How to apply?

1. Apply variational principle: What's the best possible set of \(\psi_i\)?  We'll say best are the set that give the lowest expectation value of energy.
       \begin{eqnarray}
       \langle E \rangle & = & \langle \Psi | \hat{H} | \Psi \rangle /\langle \Psi | \Psi\rangle \\
       \frac{ \delta \langle E \rangle}{\delta \psi_i} &= &0, \forall i
       \end{eqnarray}
2. Lagrange multipliers to impose orthonormality constraint on \(\psi_i\):
   \begin{eqnarray}
     \langle \psi_i | \psi_j \rangle &=& \delta_{ij} \\
      L & = & \langle E \rangle - \sum_{i,j}\epsilon_{ij}\left ( \langle \psi_i | \psi_j \rangle - \delta_{ij} \right ) \\
    \delta L &=& 0
    \end{eqnarray}
3. Coupled, one-electron Hartree eigenvalue equations for energy-optimal \(\psi_i\):
    \begin{eqnarray}
     \left \{ \hat{h}_i + \hat{v}^{\text{Hartree}}_i \right \} \psi_i(\mathbf{r}_1) &=&         \epsilon_i      \psi_i(\mathbf{r}_1) \\
       \hat{v}^{\text{Hartree}}_i(\mathbf{r}_i) & =& \sum_{j\ne i} \int \left | \psi_j(\mathbf{r}_2) \right      |^2\frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2
           \end{eqnarray}
Have to solve this for all /n/ electrons of an atom/molecule. "Hartree potential" represents Coulomb repulsion between electron /i/ and all other electrons, /averaged/ over position of those electrons.  Always positive.  This is a /mean field/ approximation.  Note appearance of "one electron" energies, \(\epsilon_i\), kinetic energy plus repulsion of electron with all others.  Total energy is sum of these \(\epsilon_i\) corrected to avoid overcounting repulsions:
\[ \langle E \rangle = \sum_i \epsilon_i - \frac{1}{2}\sum_i \langle \psi_i| \hat{v}^{\text{Hartree}}_i|\psi_i\rangle \]

Presents an obvious difficulty.  If we don't know \(\psi_j\) ahead of time, how can we even construct Hartree equations, let alone solve them?  Hartree offered a numerical solution, in the 1930's, called the */self-consistent field/ (SCF) approach*:
1. Guess an initial set of \(\psi_i\), one for each electron (he did this on a grid, and
   jumping ahead a bit, allowed each \(\psi_i\) to represent two electrons)
2. Construct Hartree potential for each \(\psi_i\)
3. Solve the \(n\) differential equations for \(n\) new \(\psi_i\)
4. Compare new to old \(\psi_i\)
5. If the same within a desired tolerance, you are done!
6. If not, return to step 2, using new \(\psi_i\), and repeat.

Hartree's father did this by hand for all the atoms of the periodic table, tabulating wavefunctions and energies for all the electrons in each.  See Hartree, Douglas R. /The Calculation of Atomic Structures/ (1957). For instance, for He, he'd solve one equation, self-consistently, to get one \(\psi_1\), and then combine to get \(\Psi(1,2) = \psi_1(1)\alpha(1)\psi_1(2)\beta_2\).  Tedious!  Qualitatively great, quantitatively not so hot.  Mean-field approximation just not so hot.

Nonetheless, basic idea of representing many-body wavefunction in terms of "orbitals," of setting up orbital equations, and solving using a self-consistent procedure, remain today at the heart of virtually all electronic structure calculations.  Hurrah Hartree!

#+BEGIN_QUOTE
/Note: It would be very cool to write a simple Python code to illustrate the SCF procedure for two electrons in an atom. Could be done on a grid or in a basis. See eg [[http://www.users.csbsju.edu/~frioux/scf/scf-he.pdf]]./
#+END_QUOTE

** The Pauli principle
One big conceptual short-coming of the Hartree model is that it treats the electrons as if they were distinguishable. QM says electrons are indistinguishable.  Furthermore, they have a quantized angular momentum, called a spin, that is either up or down, making them fermions.  
#+BEGIN_QUOTE
*Pauli principle*: The wavefunction of a multi-particle fermion system must be anti-symmetric to coordinate exchange.
#+END_QUOTE
\[\Psi(\bm{x}_1, \bm{x}_2) = -\Psi(\bm{x}_2, \bm{x}_1) \]

Here the coordinate *x* includes both the position and the spin (up or down, \alpha or \beta) of the electron.

Sorry Hartree.  Can fix for He by writing 
\[\Psi(\bm{x}_1,\bm{x}_2) = \psi_1(\bm{r}_1)\psi_1(\bm{r}_2)\left(\alpha(1)\beta(2) - \beta(1)\alpha(2)\right) \]
Hey, gentle reader, check, does this work?  Yes! Exchanging the coordinates changes the sign but keeps everything else the same.  Normalizing is easy if we take \(\psi_1\) to be normalized and recall that spin functions are orthogonal:
\[\Psi(\bm{x}_1,\bm{x}_2) = \frac{1}{\sqrt{2}}\psi_1(\bm{r}_1)\psi_1(\bm{r}_2)\left(\alpha(1)\beta(2) - \beta(1)\alpha(2)\right) \]

Note it is impossible to construct an antisymmetric wavefunction in which both electrons have the same spatial function /and/ the same spin.  Two electrons cannot have the same space and spin variables.
#+BEGIN_CENTER
#+ATTR_LATEX: :width 0.25\textwidth
[[./Images/Pauli.png]]
#+END_CENTER

** Slater determinants and Hartree-Fock
Slater determinant a general way to assure that a wavefunction satisfies Pauli principle:
\[ \Psi = \frac{1}{\sqrt{n!}}\left | \begin{array}{cccc}
\psi_1(1) & \psi_2(1) & \cdots & \psi_n(1) \\
\psi_1(2) & \psi_2(2) & \cdots & \psi_n(2) \\
\vdots & \vdots & \ddots & \vdots \\
\psi_1(n) & \psi_2(n) & \cdots & \psi_n(n) \end{array} \right | = |\psi_1\psi_2\cdots\psi_n\rangle \]
Swapping rows swaps coordinates and, by rules of determinants, changes sign.

Let's compare.  Two spin-paired electrons in two different orbitals:
\[ \left | \begin{array}{cc}
\psi_1(1)\alpha(1) & \psi_2(1) \beta(1) \\
\psi_1(2)\alpha(2) & \psi_2(2) \beta(2)\end{array} \right |
 = \psi_1(1)\psi_2(2)\alpha(1)\beta(2)-\psi_2(1)\psi_1(2)\beta(1)\alpha(2) \]
Antisymmetric?  What happens when the two electrons have the same spatial coordinate?

Two spin-aligned electrons in two different orbitals:
\[ \left | \begin{array}{cc}
\psi_1(1)\alpha(1) & \psi_2(1) \alpha(1) \\
\psi_1(2)\alpha(2) & \psi_2(2) \alpha(2) \end{array} \right |
 = \left (\psi_1(1)\psi_2(2)-\psi_2(1)\psi_1(2)\right ) \alpha(1)\alpha(2) \]
What happens now?

Exchange guarantees that two electrons of same spin cannot be in the same place!  No such guarantee for electrons of opposite spin.
** Hartree-Fock equation
Same song and dance:
1. Apply variational principle to Slater determinant. The best \(\psi_i\) are those that minimize the expectation value of the energy.
2. Use method of Lagrange multipliers to keep \(\psi_i\) orthogonal.

For simplicity, restrict ourself to cases with an even number of electrons /N/, all spin-paired, so two electrons in every orbital.  Let index /j/ run over all occupied orbitals. Arrive at *restricted Hartree-Fock* equations:
    \begin{eqnarray}
     \left \{ \hat{h}_i + \hat{v}^{\text{Hartree}}_i \right + \hat{v}^{\text{exchange}}\} \psi_i(\mathbf{r}_1) &=&         \epsilon_i      \psi_i(\mathbf{r}_1) \\
       \hat{v}^{\text{Hartree}}_i(\mathbf{r}_1) & =& 2 \sum_{j\ne i} \int \left | \psi_j(\mathbf{r}_2) \right      |^2\frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2 \\
       \hat{v}^{\text{exchange}}_i(\mathbf{r}_1) \psi_i(\mathbf{r}_1) & = &-\psi_j(\mathbf{r}_1) \sum_{j\ne i} \int \psi_j (\mathbf{r}_2) \cdot \psi_i (\mathbf{r}_2)  \frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2
           \end{eqnarray}

Yikes! Slater determinant wavefunction results in appearance of the "exchange" operator, which turns a \(\psi_i\) into a \(\psi_j\).  Exchange operator is not a simple multiplication.  Must be solved self-consistently, and is much harder to do than the simple Hartree expression. 

Slight simplification possible, noting that \(i=j\) terms cancel out and slightly redefining operators:
    \begin{eqnarray}
     \left \{ \hat{h}_i + \hat{v}^{\text{Hartree}}_i \right + \hat{v}^{\text{exchange}}\} \psi_i(\mathbf{r}_1) &=&         \epsilon_i      \psi_i(\mathbf{r}_1) \\
       \hat{v}^{\text{Hartree}}(\mathbf{r}_1) & =& 2 \sum_{j} \int \left | \psi_j(\mathbf{r}_2) \right      |^2\frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2 \\
       \hat{v}^{\text{exchange}}_i(\mathbf{r}_1) \psi_i(\mathbf{r}_1) & = &-\psi_j(\mathbf{r}_1) \sum_{j} \int \psi_j (\mathbf{r}_2) \cdot \psi_i (\mathbf{r}_2)  \frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2
           \end{eqnarray}

Now ``Hartree potential'' is the same for all orbitals/electrons. We can define the ``charge density'' to be
\[ \rho(\mathbf{r}) = 2 \sum_j |\psi_j(\mathbf{r})^2|\]
(units of charge/unit volume, multiply by /e/ to get a charge). The Hartree potential can be written
\[ \hat{v}^{\text{Hartree}}(\mathbf{r}_1) = \int\frac{\rho(\mathbf{r}_2)}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2 \]
This is the Coulomb repulsion of an electron with all electrons, /including itself/!  Called Poisson equation, well known in classical physics.  Because it involves a Coulomb repulsion, will see either \(\hat{v}^{\text{Hartree}}\) or \(\hat{v}^{\text{Coulomb}}\).  I'll often write \(\hat{v}^{\text{Coulomb}}[\rho(\mathbf{r})]\), to emphasize that the Coulomb potential is a /functional/ of the charge density.

(Just to make sure we are following units around, the Coulomb potential has units of energy/charge, eg in SI it would be J/C and would have \(e/4\pi\epsilon_0\) in front.)

The ``exchange potential'' cancels out the ``self-interaction'' of an electron with itself, and insures that two electrons of the same spin cannot be in the same place, ie, the wavefunction vanishes whenever the spatial coordinates of two electrons are the same.  It cannot be written simply in terms of the charge density.

*** Basis of wavefunction theory (WFT)
Hartree-Fock model is much better than Hartree alone, widely implemented in codes.   Not particularly good by today's standards.  However, it is
systematic and rigorous, by requiring exact adherence to the Pauli principle, and it can be systematically improved.  It
is the foundational basis of all wavefunction theory (WFT) models, all of which are characterized by exactly treating
exchange. The only approach some people call /ab initio/.

** Hartree-Fock-Slater
In 1951 John Slater introduced an approximation to the Hartree-Fock model that turned out to anticipate a whole new approach to solving the electronic structure problem, called /density functional theory/.  

Rewrite exchange part as (and shorten ``exchange'' to ``x''):
    \begin{eqnarray}
       \hat{v}^{\text{x}}_i(\mathbf{r}_1) \psi_i(\mathbf{r}_1) & = &-\psi_j(\mathbf{r}_1) \sum_{j} \int \psi_j (\mathbf{r}_2) \cdot \psi_i (\mathbf{r}_2)  \frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2 \\
   & = & -\psi_j(\mathbf{r}_1) \sum_{j} \int \psi_j (\mathbf{r}_2) \cdot \psi_i (\mathbf{r}_2)  \frac{1}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2 \cdot \frac{\psi_i(\mathbf{r_1})\psi_i(\mathbf{r_1})}{\psi_i(\mathbf{r_1})\psi_i(\mathbf{r_1})} \\
   & = & -\left [ \int \frac{\rho_i^{\text{x}}(\mathbf{r}_1;\mathbf{r}_2)}{|\mathbf{r}_2-\mathbf{r}_1|}d\mathbf{r}_2 \right ] \psi_i(\mathbf{r}_1) \\
  \rho_i^{\text{x}}(\mathbf{r}_1;\mathbf{r}_2)& =& \sum_j \frac{\psi_i(\mathbf{r_1})\psi_j(\mathbf{r_2})\psi_j(\mathbf{r_1})\psi_i(\mathbf{r_2})}{\psi_i(\mathbf{r_1})\psi_i(\mathbf{r_2})}
           \end{eqnarray}

This looks like the Coulomb expression, but the density thing is different for each orbital /i/.  The ``exchange density'' does have units of charge density enters in the same way, but with minus sign, to the electron density. Suggests that exchange can be thought of as an electron ``hole'' around an electron.  This exchange density has some special properties:

1. Every electron at any position \(\mathbf{r}_1\) has an exchange hole around it equal to one electron of the same spin as itself:
   \[ \int \rho_i^{\text{x}}(\mathbf{r}_1;\mathbf{r}_2) d\mathbf{r}_2 = 1 \]
2. The exchange hole exactly cancels out all electrons of the same spin at the electron location.  Ie, it exactly fixes self-interaction:
   \[\rho_i^{\text{x}}(\mathbf{r}_1;\mathbf{r}_1) = \sum_j |\psi_j(\mathbf{r})^2|\]

Thus, the Coulomb repulsion felt by an electron is diminished by an exchange hole that
follows the electron around, exactly canceling out the charge at its current location.
It’s not necessarily spherical and is not the same for all orbitals, but the fact that it
has these general properties gives hope that it can be approximated somehow.

Hey, I have an idea!  (Actually, Slater had an idea.) What if we had a homogeneous (density the same everywhere) gas of electrons, like electrons of a given density \(\rho\) in an infinite box?  By symmetry the exchange hole would be spherical, and if it must integrate to 1, then it must have a radius (factor of 2 comes from fact we are only including electrons of the /same/ spin):
\[R_\text{hole} = \left [\frac{4\pi\rho/2}{3} \right ]^{1/3} \]
The potential felt by an electron due to this spherical hole is
\[\hat{v}^\ext{x} = -\frac{1}{2}\int_\text{sphere}\frac{\rho}{r}dr = - \left [\frac{9\pi\rho}{4}\right ]^{1/3} \]

Now, let's assume that an electron in a real system experiences an exchange hole potential at any point exactly like that of a homogeneous electron gas of the same density at that point.  This is the basis of the /Hartree-Fock-Slater/ model:
\[\hat{v}^\ext{x,HFS}(\mathbf{r}_1) =-\frac{3}{2}\left[\frac{3\rho(\mathbf{r}_1)}{\pi} \right] -C\rho(\mathbf{r}_1)^{1/3} \]
Some ambiguity as to the right value of the constant /C/, so sometimes just taken as a parameter.

Can now write the /Hartree-Fock-Slater/ equation:
\[ \left\{ \hat{h} + \hat{v}^\text{Coulomb}[\rho] + \hat{v}^\text{x,HFS}[\rho]\right \} \psi_i(\mathbf{r}) = \epsilon_i\psi_i(\mathbf{r}) \]
This is *much* simpler to solve than Hartree-Fock equation, because the left hand side is the same for all electrons given a total density \(\rho(\mathbf{r})\). Still must be solved iteratively, using the /self-consistent field/.

*** Notes
1. Exchange potential scales with the total number of electrons around: more electrons (like near a nucleus) means a more compact, denser exchange hole, more electron exchange ``screening,'' a greater decrease in potential energy.  Further from nucleus, more diffuse exchange hole, less screening. 

2. Screening is not exact, though; does not exactly cancel self-interaction.  Clearest in one-electron case: Coulomb and exchange potentials should then exactly cancel, which they evidently do not!  HFS energy of an H atom is not exactly −0.5 au!

3. From a computational point of view, the exchange potential goes from being the hard thing to evaluate to being the easy thing.  The Coulomb potential takes more effort to evaluate, and tricks are often implemented to simplify that, like fitting the density to an auxiliary basis set.  On the other hand, the 1/3 power makes the exchange potential non-analytic, and solution of the HFS equation (and all DFT methods) involves some form of numerical quadrature.

4. How does the HFS model do?  Pretty darn well, in particular for calculating the structures of things, and it works nicely for things like metals.  Not a bad deal!  Way to go, Slater!

5. Another aside: back in the day, the numerical implementations of X\alpha were very crude and sometimes gave unreasonable results (like linear water!).  Slater still sold it very hard, which did not enamor him or all of DFT to the chemical community, although the physics community was far more accepting.  For many years DFT was unaccepted by chemists, until solid numerical implementations in codes like =Gaussian= brought it to the mainstream.
*** Basis of density functional theory (DFT)
Slater's arguments are not rigorous. However, as we will see later, they can be made rigorous.  HFS is the very simplest example of a /density functional theory/ model, because it is a model built entirely on charge density.  Such approach is justifiable.

** Implementations
*** =GAMESS=
Hartree-Fock method /always/ paired with /basis set/ methods and implemented in the codes available at [[http://webmo.net]].  Example =GAMESS= input for Hartree-Fock Ar:

#+BEGIN_EXAMPLE
 $CONTRL SCFTYP=RHF RUNTYP=ENERGY ISPHER=1
       ICHARG=0 MULT=1 COORD=CART $END
 $BASIS GBASIS=CCT $END
 $DATA
Ar
OH 1

Ar 18 0.00000000 0.00000000 0.00000000
 $END
 #+END_EXAMPLE

And for Hartree-Fock-Slater Ar:

#+BEGIN_EXAMPLE
 $CONTRL SCFTYP= RHF RUNTYP=ENERGY DFTTYP=Slater ISPHER=1
       ICHARG=0 MULT=1 COORD=CART $END
 $BASIS GBASIS=CCT $END
 $DATA
Ar
OH 1

Ar 18 0.00000000 0.00000000 0.00000000
 $END
#+END_EXAMPLE

*** =FDA=
Hartree-Fock-Slater is intrinsically numerical.  Historically interesting is the Herman-Skillman code, that solves the problem numerically on a grid.  Available to us as the =fda= code, see [[https://www.chemsoft.ch/qc/fda.htm]] and [[./Resources/00READ.ME]].

Ar input:
#+BEGIN_EXAMPLE
300 0.0001 30.0
50 0.00001 0.10 0.50  0.682 0.0042
18.0 5
1 0 1.0 1.0
2 0 1.0 1.0
2 1 3.0 3.0
3 0 1.0 1.0
3 1 3.0 3.0
#+END_EXAMPLE

Output at [[./Resources/Ar.out]].

#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np

# Lets open the file in read mode
with open('./Resources/Ar.dmp', 'r') as f:

    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()

    # First we read the grid points and the total charge densities
    grid_points = []
    total_charge_densities = []

    for line in lines[3:303]:

        # Each is a string with two columns
        grid_point, tot_charge_density = line.split()

        # We need to convert each line to a float add it to our lists
        grid_points.append(float(grid_point))
        total_charge_densities.append(float(tot_charge_density))
    
    # Now for the 1s orbital
    one_s = []
    one_s = [float(x) for x in lines[304:604]]

    two_s = []
    two_s = [float(x) for x in lines[605:905]]

    two_p = []
    two_p = [float(x) for x in lines[906:1206]]

    three_s = []
    three_s = [float(x) for x in lines[1207:1507]]

    three_p = []
    three_p = [float(x) for x in lines[1508:1808]]
#    for x in lines[304:604]:
#        one_s_charge_density.append(float(x))
 

plt.figure()
#plt.semilogx(grid_points, total_charge_densities)
plt.plot(grid_points, total_charge_densities)
plt.xlabel('Grid Points')
plt.ylabel('Charge Density')
plt.title('Overall')
plt.savefig('./Images/Ar-overall-charge-density.png')

plt.figure()
plt.plot(grid_points, one_s,label='1s')
plt.plot(grid_points, two_s,label='2s')
plt.plot(grid_points, two_p,label='2p')
plt.plot(grid_points, three_s,label='3s')
plt.plot(grid_points, three_p,label='3p')
plt.legend()
plt.xlim(0,6)

plt.xlabel('Distance (bohr)')
plt.ylabel('Wavefunction rR(r)')
plt.title('Ar radial wavefunctions')
plt.savefig('images/Ar-wave-functions.png')
#plt.show()
#+END_SRC

#+RESULTS:
#+ATTR_LATEX: :width 0.8\textwidth
[[./Images/Ar-wave-functions.png]]

** Performance
One metric is the ability to predict ionization energies.  
#+BEGIN_QUOTE
_Koopman’s theorem_: The negative of the energy of an occupied orbital (\(-\epsilon_i)\)) approximates the energy to extract an electron from that orbital, ie to ionize the system.   The energy of a virtual orbital approximates the energy to add an additional electron to a system, i.e. the electron affinity.  Assumes no relaxation of orbitals.  
#+END_QUOTE

#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np

#fig = plt.gcf()
aueV = 27.2114

Atom = ["He", "Ne", "Ar", "Kr"]
IPexpt = np.array([24.59, 21.56, 15.76,14.00])
IPXalpha = np.array([0.8998,0.7912,0.5735,0.5130]) # Hartree
IPXalphaeV = IPXalpha*aueV

offset = -0.7
plt.figure()
for i in [0,1,2,3]:
    plt.scatter(IPexpt[i],IPXalphaeV[i],color='black')
    plt.text(IPexpt[i],IPXalphaeV[i]+offset,Atom[i],color='black')
plt.plot([13,25],[13,25],color='black',linestyle=':',label='y=x')
plt.xlabel('Experiment (eV)')
plt.ylabel('DFT (eV)')
plt.legend()

plt.title('DFT-computed vs experimental 1st ionization energies')

plt.savefig('./Images/Ionization.png')

#+END_SRC

#+RESULTS:

#+CAPTION: HFS vs exact ionization energies
#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/Ionization.png]]

** Correlation
If solved to reasonable precision, both the Hartree-Fock and Hartree-Fock-Slater models work pretty darn well for things like the shapes of molecules, structures of solids, charge distributions, vibrational frequencies, ....  Don't work so well for computing things that involve making and breaking bonds, like a reaction energy or an activation energy.

Why?  All the models discussed here neglect /electron correlation/, the fact that the
potential felt by an electron /is/ a function of the instantaneous positions of all the
other electrons.  The contribution of correlation to absolute energies is not big by
proportion, but it is very imporant to energy differences.  Any ``orbital'' model cannot
capture correlation.  It can be introduced systematically and exactly into H-F models (at
great computational expense) and systematically and approximately into DFT models (at much
more modest expense). Hence the popularity of DFT!

\newpage
* Practical electronic structure
** Born-Oppenheimer approximation
In principle all nuclei and electrons should be described quantum mechanically.  For \ce{H2},
for instance, true wavefunction would be a function of the positions of nuclei and
electrons, $\Upsilon(\mathbf{r}_{1}, \mathbf{r}_{2},\mathbf{R}_{\alpha},\mathbf{R}_{\beta})$.

#+ATTR_LATEX: :width 0.3\textwidth
[[./Images/BornOppenheimer.png]]

Nuclei much heavier than electrons and move much more slowly.  Assume nuclei are fixed in
space ("clamped") and electrons move in static field of those electrons. Equivalent to
assuming that nuclear kinetic energy is decoupled from electron dynamics.  Only change is that
\[\hat{h} = -\frac{1}{2}\nabla^2 - \sum_\alpha \frac{Z_\alpha}{|\bm{r}-\bm{R}_\alpha|} \]

Schr\ouml{}dinger equation becomes parameteric in nuclear positions; solutions
$E(\mathbf{R}_{\alpha},\mathbf{R}_{\beta})$ define a potential energy surface (PES).

\[E_\text{PES}(\mathbf{R}_{\alpha},\mathbf{R}_{\beta}) = E_\text{Schr} +\frac{1}{2}\sum_{\alpha,\beta}\frac{Z_\alpha Z_\beta }{|\mathbf{R}_{\beta} - \mathbf{R}_{\alpha}|}\]

#+ATTR_LATEX: :width 0.5\textwidth
[[./Images/PES.png]]

** Model chemistry
Essentially always start with
\begin{equation}
\left \{ \hat{h} +v_{\text{Coulomb}}[\rho] + v_\text{exchange}[\psi_{i}] + v_\text{correlation}[\psi_{i}]\right\}\psi_i(\mathbf{r}) =\epsilon_i \psi_i(\mathbf{r})
\end{equation} 
label:fock

Standard models of today all treat the one-electron and Coulomb pieces exactly and treat
the electron-electron interactions at various levels of approximation.
|------------------------------------+----------------------------------+----------------------------------+---------------------------------|
|                                    | $v_{\text{exchange}}$            | $v_{\text{correlation}}$         |                                 |
|------------------------------------+----------------------------------+----------------------------------+---------------------------------|
| *Wave function theory* (WFT)       |                                  |                                  |                                 |
| Hartree                            | self-interaction                 | neglect                          | historic                        |
| Hartree-Fock                       | exact                            | neglect                          | superceded                      |
| MPn, CC                            | exact                            | perturbative                     | state-of-the-art                |
| CI                                 | exact                            | variational                      | specialized                     |
|------------------------------------+----------------------------------+----------------------------------+---------------------------------|
| *Density functional theory* (DFT)  |                                  |                                  |                                 |
| Hartree-Fock-Slater                | $[\rho^{{4/3}}]$                 | neglect                          | historic                        |
| Local density approximation        | $[\rho^{{4/3}}]$                 | $[\rho]$                         | general purpose solids          |
| (LDA)                              |                                  |                                  |                                 |
| Generalized gradient approximation | $[\rho,\nabla\rho]$              | $[\rho,\nabla\rho]$              | general purpose solids/surfaces |
| (GGA)                              |                                  |                                  |                                 |
| "Improved" GGA                     | $[\rho,\nabla\rho]$              | $[\rho,\nabla\rho]$              | general purpose                 |
| (RPBE, BEEF, Mxx)                  |                                  |                                  |                                 |
| Hybrid                             | $\approx$ exact                  | $[\rho,\nabla\rho]$              | general purpose molecules       |
| (B3LYP, PBE0, HSE06)               |                                  |                                  | specialty solids/surfaces       |
| Meta GGA                           | $[\rho,\nabla\rho,\nabla^2\rho]$ | $[\rho,\nabla\rho,\nabla^2\rho]$ | developing                      |
|------------------------------------+----------------------------------+----------------------------------+---------------------------------|

The choice of the electronic structure model is the most fundamental approximation in
applying these methods.  Determined from experience and need.

Specification in =GAMESS= ([[https://www.msg.chem.iastate.edu/GAMESS/GAMESS.html]]) is a bit arcane.  Default is Hartree-Fock. To specify DFT model, use
#+BEGIN_EXAMPLE
 $CONTRL DFTTYP =   Slater (HFS), SVWN (LDA), PBE (GGA), B3LYP (Hybrid)
#+END_EXAMPLE
** Bring back the basis sets
The one-electron equations eq ref:fock give us defining expressions for the energy-optimal
orbitals, but they aren't convenient to solve for anything more complicated than an atom. Expand solutions in a basis set:
\[\psi_i(\bm{r}) = \sum_\nu C_{\mu i}\phi_\nu(\bm{r}) \]
Often atom-centered.  You'll see the term "linear combination of atomic orbitals," LCAO.

Abbreviate \(\hat{f}\psi_i = \epsilon_i\psi_i\]. Substitute in \(\psi_i\), mulitple through by a basis function \(\phi_\mu\):
\[\sum_\nu F_{\mu \nu} C_{\nu i} = \epsilon_i \sum_\nu S_{\mu \nu} C_{\nu i}, \qquad \bm{FC} = \bm{SC}\epsilon \]
where
\[ F_{\mu \nu} = \langle \psi_\mu|\hat{f}|\phi_\nu\rangle\qquad  S_{\mu \nu} = \langle \psi_\mu|\phi_\nu\rangle\]
Matrix equation to solve.

Historically interesting, ``semi-empirical'' methods (MNDO, \ldots) worked by parameterizing the matrix elements against atom properties.

Recall \(\hat{f}\) depends on the density, which can be written
\[ \rho(\bm{r}) = \sum_{\mu \nu} P_{\mu \nu}\phi_\mu(\bm{r})\phi_\nu(\bm{r}),\qquad P_{\mu \nu}=2\sum_i C_{\mu i} C_{\nu i}  \]
Depending on implementation, pieces of \(\hat{f}\) can often be computed just once and reused, eg one-electron integrals \(\langle\phi_\mu|\hat{h}|\phi_\nu\rangle\).

*Algorithm*:
1. Put your atoms somewhere in space
2. Select a basis
3. Pre-compute what you can
4. Guess some coefficients/density/density matrix
5. Construct secular matrix elements
6. Solve secular matrix equation for \(C\) and \(\epsilon\)
7. Construct and compare new density to old
8. Update density and repeat, or \ldots
9. \ldots if less than tolerance, all done!

*ALWAYS* check to be sure result has converged, to the state you want!

** H2O Energy Example
Hartree-Fock calculation on \ce{H2O}, minimal (STO-3G) basis set.
#+BEGIN_EXAMPLE
!   File created by the GAMESS Input Deck Generator Plugin for Avogadro
 $BASIS GBASIS=STO NGAUSS=3 $END
 $CONTRL SCFTYP=RHF RUNTYP=ENERGY COORD=CART $END
 $DATA 
Title: H2O energy evaluation
C1
O     8.0    -0.89600     3.13196     0.00000
H     1.0     0.07400     3.13196     0.00000
H     1.0    -1.21933     3.71670     0.70316
 $END
#+END_EXAMPLE

See [[./Resources/H2O-STO3G.gamout]].
** Symmetry
Often problem can be simplified by taking advantage of symmetry of the system.
#+BEGIN_EXAMPLE
!   File created by the GAMESS Input Deck Generator Plugin for Avogadro
 $BASIS GBASIS=STO NGAUSS=3 $END
 $CONTRL SCFTYP=RHF RUNTYP=ENERGY COORD=CART $END
 $DATA 
Title: H2O energy evaluation
CNV 2

O     8.0    -0.89600     3.13196     0.00000
H     1.0     0.07400     3.13196     0.00000
H     1.0    -1.21933     3.71670     0.70316
 $END
#+END_EXAMPLE

Results get labeled by symmetry labels.

See [[./Resources/H2O-C2V.gamout]].

** Dissociating H2+ example
Compute energy vs distance.  Should dissociate to H atom and \ce{H+} ion.
#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np

H_atom_HF = -0.46658185
H_atom_LDA = -0.4356702
He_cation_HF = -1.9317484483
He_atom_HF = -2.80778396
He_atom_LDA = -2.771886

plt.figure()

baseline = H_atom_HF
with open('./Resources/H2+-HF-STO3G.dat', 'r') as f:
    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()
    distance = []
    HF_energy = []

    for line in lines:
        # Each is a string with two columns
        dist, energy = line.split()

        # We need to convert each line to a float add it to our lists
        distance.append(float(dist))
        HF_energy.append(float(energy)-baseline)

plt.plot(distance,HF_energy, label='H-F')

baseline = H_atom_LDA
with open('./Resources/H2+-LDA-STO3G.dat', 'r') as f:
    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()
    distance = []
    LDA_energy = []

    for line in lines:
        # Each is a string with two columns
        dist, energy = line.split()

        # We need to convert each line to a float add it to our lists
        distance.append(float(dist))
        LDA_energy.append(float(energy)-baseline)

plt.plot(distance,LDA_energy,label='LDA')
plt.plot([0.,4.0],[0.,0.],ls='--',color='black')
plt.xlim(0.4,4.0)

plt.legend()
plt.xlabel('distance (Angstrom)')
plt.ylabel('Energy (Hartree)')
plt.title('H2+ Energies vs distance, referenced to H')
plt.savefig('./Images/H2+.png')

#+END_SRC

#+RESULTS:
#+ATTR_LATEX: :width 0.7\textwidth
[[./Images/H2+.png]]

Oops, come on, LDA!  Illustrates self-interaction problem in LDA.  Electron is too eager to be diffuse, spreads out over both atoms when it should localize on one.

** Dissociating HHe+ example
Compare Hartree-Fock and LDA for \ce{H-He+} vs distance. (Isoelectronic to \ce{H2}, but avoids any problems with symmetry. Should dissociate to \ce{H+} and He.  Does it?

#+BEGIN_EXAMPLE
 $BASIS GBASIS=STO NGAUSS=3 $END
 $CONTRL SCFTYP=RHF RUNTYP=ENERGY ICHARG=1 MULT=1 $END
 $DATA 
Title
C1
H     1.0     0.   0.  0.
He    2.0     0.   0.  XXX
 $END
#+END_EXAMPLE

#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np

H_atom_HF = -0.46658185
He_cation_HF = -1.9317484483
He_atom_HF = -2.80778396
He_atom_LDA = -2.771886

plt.figure()

baseline = He_atom_HF
with open('./Resources/HHe+-HF-STO3G.dat', 'r') as f:
    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()
    distance = []
    HF_energy = []

    for line in lines:
        # Each is a string with two columns
        dist, energy = line.split()

        # We need to convert each line to a float add it to our lists
        distance.append(float(dist))
        HF_energy.append(float(energy)-baseline)

plt.plot(distance,HF_energy, label='H-F')

baseline = He_atom_LDA
with open('./Resources/HHe+-LDA-STO3G.dat', 'r') as f:
    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()
    distance = []
    LDA_energy = []

    for line in lines:
        # Each is a string with two columns
        dist, energy = line.split()

        # We need to convert each line to a float add it to our lists
        distance.append(float(dist))
        LDA_energy.append(float(energy)-baseline)

plt.plot(distance,LDA_energy,label='LDA')
plt.plot([0.,3.0],[0.,0.],ls='--',color='black')
plt.xlim(0.4,3.)

plt.legend()
plt.xlabel('distance (Angstrom)')
plt.ylabel('Energy (Hartree)')
plt.title('HHe+ Energies vs distance, referenced to H+ and He')
plt.savefig('./Images/HHe+.png')

#+END_SRC

#+RESULTS:
#+ATTR_LATEX: :width 0.7\textwidth
[[./Images/HHe+.png]]

Equilibrium distance? How's the dissociation state?  Bond energy?  Truth is about \SI{-0.075}{Hartree}. LDA has advantage of cancellation of errors between exchange and correlation errors.  A good thing!

** Dissociated He2 example
#+BEGIN_SRC python :exports results
import matplotlib.pyplot as plt
import numpy as np

H_atom_HF = -0.46658185
He_cation_HF = -1.9317484483
He_atom_HF = -2.80778396
He_atom_LDA = -2.771886

plt.figure()

baseline = 2.* He_atom_HF
with open('./Resources/He2-HF-STO3G.dat', 'r') as f:
    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()
    distance = []
    HF_energy = []

    for line in lines:
        # Each is a string with two columns
        dist, energy = line.split()

        # We need to convert each line to a float add it to our lists
        distance.append(float(dist))
        HF_energy.append(float(energy)-baseline)

plt.plot(distance,HF_energy, label='H-F')

baseline = 2.* He_atom_LDA
with open('./Resources/He2-LDA-STO3G.dat', 'r') as f:
    # Reading all the lines in the file
    # Each line is stored as an element of a list
    lines = f.readlines()
    distance = []
    LDA_energy = []

    for line in lines:
        # Each is a string with two columns
        dist, energy = line.split()

        # We need to convert each line to a float add it to our lists
        distance.append(float(dist))
        LDA_energy.append(float(energy)-baseline)

plt.plot(distance,LDA_energy,label='LDA')
plt.plot([0.,3.0],[0.,0.],ls='--',color='black')
plt.xlim(0.4,3.)

plt.legend()
plt.xlabel('distance (Angstrom)')
plt.ylabel('Energy (Hartree)')
plt.title('He2 Energies vs distance, referenced to He')
plt.savefig('./Images/He2.png')

#+END_SRC

#+RESULTS:

[[./Images/He2.png]]
#+RESULTS:


** Open-shell systems
First, some jargon related to unpaired electrons:
| # unpaired electrons | $S$   | $2S+1$ | name    |
|----------------------+-------+--------+---------|
|                    0 | 0     |      1 | singlet |
|                    1 | $1/2$ |      2 | doublet |
|                    2 | 1     |      3 | triplet |
|                    3 | $3/2$ |      4 | quartet |

Model has to be generalized somewhat to deal with systems with unpaired electrons. One approach is to construct wavefunctions that are exactly spin-adapted (eigenfunctions of the \(\hat{S}\) operator). Possible in the Hartree-Fock world, but messy.  More common is to relax that constraint a bit, define different orbital wavefunctions for spin-up and spin-down electrons, called /unrestricted/ or /spin-polarized/ (opposite of /non-spin-polarized/!).  Means that electron density has different spin-up and spin-down parts.

#+ATTR_LATEX: :width 0.9\textwidth
[[./Images/Polarization.png]]

Controlled in =GAMESS= using the =$CONTRL= group:
#+BEGIN_EXAMPLE
 $CONTRL SCFTYP = RHF   non-spin-polarized, default
         SCFTYP = UHF   spin-polarized
         MULT   = 1 (default), 2,...  spin multiplicity
         ICHARG = 0 (default), 1,...  net charge
 $END
#+END_EXAMPLE

** Gaussian basis sets
Gaussian functions ($e^{-\zeta|\mathbf{r}|^2}$) are the most popular choice for atom-centered basis sets.  They do not efficiently represent molecular wavefunctions, but one- and two-electron integrals in  WFT  can be solved analytically over Gaussians.

Other choices, like Slater functions ($e^{-\zeta|\mathbf{r}|}$) are possible but require numerical quadrature.

Gaussian basis sets have to be created for any given atom and must be used consistently within a set of calculations.

- _Primitive_ is a single Gaussian function, possibly multipled by a polynomial to look like
  /s/, /p/, \ldots.  Defined by an exponent $\zeta$ that determines how extensive (small $\zeta$) or compact
  the function is.
- _Contraction_ is a pre-set linear combination of several primitive Gaussians.
- _Basis set_ is a predefined set of exponents and contraction coefficients appropriate
  for some specific atom.

*** Gaussian basis set nomenclature
- _Minimal basis_ contains one contracted function for every atomic orbital. STO-3G is the
  poster child.
- _Double zeta_ contains two contracted functions for every atomic orbital
- _Split valence_ is more common, single zeta in core, double zete in valence, typical of Pople basis sets, eg "6-31G"
- _Triple-split valence_ would be "6-311G"
- _Polarization functions_ are functions of one angular momentum greater than the highest angular momentum occupied states, eg \(p\) function for H, or \(d\) function for C. Important to capture the polarization of charge when atoms make molecules.  Arcane nomenclature, eg 6-31G(d,p) or 6-31G**.
- _Diffuse functions_ are small exponent functions to describe anions or loosely bound electrons. Again argane nomenclature, eg 6-31+G(d,p). Yech.
- _Correlation consistent_ and _atomic natural orbital_ are series of basis sets that are constructured to be efficient and to improve systematically. 
- _Complete basis set_ (CBS) limit is notion of extrapolating energies from a series of systematically improving basis sets.  Very common in high accuracy calculations.

*** Standard basis sets in 
Specified in $BASIS group.  Some common choices, in increasing level of sophistication:

|--------------+----------------------------------+----------------------------------------|
| Name         | Type                             | Flags                                  |
|--------------+----------------------------------+----------------------------------------|
|              | *Pople type*                     | *The most venerable and widely used*   |
| STO-3G       | Minimal                          | GBASIS=STO  NGAUSS = 3                 |
| 3-21G        | Split valence                    | GBASIS=N21  NGAUSS=3                   |
| 6-31G(d)     | Split valence polarized          | GBASIS=N31 NGAUSS =6 NDFUNC=1          |
| 6-311+G(d,p) | Triple-split valence             | GBASIS=N311 NGAUSS=6 NDFUNC=1 NPFUNC=1 |
|              | polarized and augmented          | DIFFSP=1                               |
|              |                                  |                                        |
|              | *Polarization-consistent*        | *Good for DFT*                         |
| PC0          | Minimal                          | GBASIS=PCseg-0                         |
| PC1          | Split valence polarized          | GBASIS=PCseg-1                         |
| PC2          | Triple split double polarized    | GBASIS=PCseg-22                        |
|              |                                  |                                        |
|              | *Correlation-consistent*         | *Good for MP2 and beyond*              |
| cc-pVDZ      | Split valence polarized          | GBASIS=CC2                             |
| cc-pVTZ      | Triple split double polarized    | GBASIS=CC3                             |
| aug-cc-pvDZ  | augmented with diffuse functions | GBASIS=ACC2                            |
|              |                                  |                                        |
|              | *Effective core potentials*      | *Good for treating heavy atoms*        |
| SBKJC        | Split valence + core potential   | GBASIS=SBKJC                           |
| Hay-Wadt     | Split valence + core potential   | GBASIS=HW                              |
|--------------+----------------------------------+----------------------------------------|

Complicated field, which is why the old standards live on in routine calculations.  Optimal approach is to employ a _composite model_, calibrated by someone else, with well defined set of basis functions and treatments of exchance and correlation.  A composite model pieces together results from a number of different calculations to estimate a higher accuracy model.

** Electron cores
Low energy "core" electrons typically don't participate in chemical bonding but can add substantially  to computational cost.  Generally seek approximations, especially  for heavy elements/metals.

Heart of approach is to partition an atom into core and valence parts.  Seek ways to
express the influence of the core on the valence without actually having to compute the
core. Essentially seek to write 
\[ \hat{v}^\text{ee} \approx \hat{v}^\text{ee,core} +
\hat{v}^\text{ee,val} \] 
where the core potential is some simpler, composite expression of
the influence of the core on the valence.  Typically take the electron cores as ``frozen''
in the pure atomic states, and express influence on valence through
 angular-momentum-dependent operators parameterized against accurate atomic calculations.  Goal is to recover valence wavefunctions with less effort.

*** Relativistic effects
Relativistic kinetic energy is relativistic total energy minus the rest energy:
 \[T = \sqrt{p^2c^2 + m_0^2c^4} - m_0c^2\]
Taylor expanding about \(p^2 = 0\) gives the _first-order mass-velocity correction_:
\[ T \approx \frac{p^2}{2m_0} - \frac{p^4}{8 m_0^3 c^2}\]
Reduces to non-relativistic result when \(c\rightarrow\infty\).  Electrons near core move at speeds close to \(c\), second term becomes non-negligible and diminishes their energy.  Most important for \(s\) states that penetrate closest to nucleus; they shield nucleus better and other valence states rise up in energy.

Electron spin and orbital magnetic moments also couple when \(l>0\), leads to _spin-orbit coupling_ that splits \(p\), \(d\), \ldots states into \(j = l \pm s\) states. 

#+CAPTION: Comparison of Non-Relativistic and Relativistic Atomic States.
#+ATTR_LATEX: :width .5\textwidth
[[./Images/Relativity.png]]

_Darwin correction_ corrects s orbitals for electron and nucleus being at the same point; comes from solution of full Dirac relativistic equation for the atom.

Relativistic effects typically incorporated implicitly, by including in model for core electrons and thus capturing their effect on the valence.  Spin-orbit, if necessary, added after the fact.
*** Implementations
Non-relativistic and relativistic effective core potentials (ECPs) available for many elements.  These specify the potential felt by the valence electrons due to the core in terms of radial potential functions and angular projection operators.  Typically these have to be combined with basis functions designed to work with them.

Most common are Hay-Wadt (LANL) and Stevens-Basch-Krause (SBK).  Other more modern ones also available, like Stuttgart.
** Population analysis
The molecular orbitals contain information that can be helpful in understanding structure and bonding.  

Perhaps most direct is the distribution of charge around the molecule, 

 
Can calculate moments of the charge (dipole, quadrupole, \ldots).  Note these are only exactly defined for neutral species.

Another interesting quantity is the Coulomb, or often-termed "electrostatic" potential (ESP) created by the combination of electrons and nuclei.  CHELPG works by trying to reproduce calculated electrostatic potential with charges placed at the atom centers.

Chemically it is intuitively nice to assign charge to individual atoms, in what is called a population analysis.  There is no single "right" way to do this\ldots the "charge" on an atom in a molecule in not uniquely defined!

Consider an occupied molecular orbital made up of two basis functions on two different atoms, \alpha and \beta:
 
 can be interpreted as the fraction of charge in ψ that can be assigned to α, and likewise  for β.  The remainder is the “overlap” population between α and β.  Who gets these?  In Mulliken population analysis they are divided equally between the two atoms.  Summing over all the occupied orbitals and subtracting from the nuclear charge gives the gross atomic populations.

Mulliken populations are directionally very useful but their absolute magnitudes have no real meaning.  They are sensitive to the choice of basis, and equal distribution of overlap density between atoms is rather arbitrary.

The overlap problem goes away if the basis is orthogonal, so the overlap terms go to zero.  In the Löwdin approach the basis set is orthogonalized according to  , so that  .  MO coefficients in this transformed basis used to assign charge to individual atoms.  More stable, less common than Mulliken approach.  This orthogonalization scheme is not unique, so other implementations possible.

Natural orbitals are another, more sophisticated scheme for orthogonalizing and assigning charge.  See http://www.chem.wisc.edu/~nbo5/.  Again more stable than Mulliken, more information rich.

Bader analysis another more sophisticated method, based on a geometric analysis of the total charge density.  See Bader, R. F. W. Atoms in Molecules: A Quantum Theory; Oxford University Press: Oxford, 1990.
** Molecular orbital (MO) diagrams
Correlate molecular orbitals with their parent fragments.  Use to be the thing.  Seldom now
** Implementation details of SCF methods
Basis is often /orthonormalized/ to eliminate overlap from H-F-R equation; allows equations to be solved by matrix diagonalization.

Initial density matrix $\mathbf{P}$ are obtained by solving an approximate Hamiltonian (like extended H\uuml{}ckel). Always beware! Initial guess can influence final converged state.

Because the number of 2-electron integrals grows as $N^4$, they are sometimes calculated as needed "on-the-fly", so-called direct SCF.

Hartree-Fock integrals can be computed analytically in a Gaussian basis.  Any other choice of basis, or any DFT functional, requires integrals to be computed by quadrature. Used to be a real hang-up.  Today, algorithms are very robust to establish grids and do quadrature.
** SCF updating
The SCF procedure is an optimization problem: find set of coefficients that minimizes the total energy. As discussed above, success depends on a reasonable initial guess for density matrix and judicious updating. Various strategies can be used to speed and stabilize convergence, like damped mixing of previous cycles.

Second-order SCF is a convergence acceleration method that requires calculation or estimation of the first- and second-derivatives of the energy with respect to the orbital coefficients. See e.g. Chaban et al., /Theor. Chem. Accts./ *1997*, /97/, 88-95.

Pulay's ``direct inversion in the iterative subspace,'' or ``DIIS,'' is a popular and powerful acceleration procedure that extrapolates from several previous Fock matrices to predict optimal next Fock to diagonalize. *An opportunity for machine learning?* 

Controlled in GAMESS using the =$SCF= group.

#+BEGIN_EXAMPLE
 $SCF DIRSCF=   .T./.F. controls direct scf
      SOSCF=    .T./.F. second-order scf
      DIIS=     .F./.T. direct inversion in the iterative subspace
      DAMP=     .T./.F. damping, on for initial iterations
 $END
#+END_EXAMPLE
\newpage
* Potential energy surfaces
The potential energy surface ("PES") is the sum of the repulsive energy of the nuclei and the kinetic and potential energies of all the electrons:
#+BEGIN_CENTER
\begin{equation}
E_\text{PES}(\mathbf{R_\alpha},\mathbf{R_\beta},\ldots) =E_\text{elec} +\sum_{\alpha=1}^N \sum_{\beta =\alpha +1}^N \frac{Z_\alpha Z_\beta e^2}{R_{\alpha \beta}}
\end{equation}
#+END_CENTER

** Specifying atomic positions

#+BEGIN_CENTER
#+ATTR_LATEX: :width 0.75\textwidth
[[./Images/Internals.pdf]]
#+END_CENTER

*** Cartesian
Computationally straightforward but don't correspond with our physical notion of bonds,
bends, etc.  Easiest to get out of a piece of software.  A molecule has \(3 N-6 \) internal degrees of freedom (\(3N-5\) if linear), but Cartesians specify \(3N\).  The extra values correspond to the location of the center of mass and molecular oriendation.  Codes will typically center and reorient the Cartesions.

In Gamess, would specify Cartesian coordinates for \ce{FCH2CH2F} like this:
#+BEGIN_EXAMPLE
 $CONTRL COORD=CART $END
 $DATA
FCH2CH2F drag calculation
C1
C     6.0    -3.76764     0.33879     0.03727
C     6.0    -2.35246     0.34495     0.03689
F     9.0    -4.72277     0.58147    -1.18012
F     9.0    -1.59909    -0.68487    -0.83662
H     1.0    -4.04387     1.08375     0.75395
H     1.0    -3.92958    -0.71060     0.16941
H     1.0    -2.03786     0.18875     1.04760
H     1.0    -2.09983     1.28759    -0.40187
 $END
#+END_EXAMPLE
*** Internal coordinates
These provide a more intuitive representation and can be convenient when building molecules by hand.  In codes like =GAMESS=, most commonly defined using "z-matrix" notation. Specify  each atom in terms of its distance, angle, and dihedral angle with three previous atoms.

In Gamess, would specify z-matrix for \ce{FCH2CH2F} like this:
#+BEGIN_EXAMPLE
$CONTRL SCFTYP=RHF RUNTYP=ENERGY COORD=ZMT $END
$DATA
FCH2CH2F drag calculation
C1
C
C   1   r1
F   2   r2   1   A1
H   2   r3   1   A2   3   D1
H   2   r4   1   A3   3   D2
F   1   r2   2   A1   3   D3
H   1   r3   2   A2   6   D1
H   1   r4   2   A3   6   D2

r1=1.5386
r2=1.39462
r3=1.11456
r4=1.12
A1=109.54214
A2=111.
A3=110.
D1=120.
D2=-120.5
D3=50.
$END
#+END_EXAMPLE
Particularly convenient when you'd like to "scan" over the value of some coordinate.  Variable can be applied to more than one independent coordinate, if the molecule has symmetry.

** Features of potential energy surfaces
#+BEGIN_CENTER
#+ATTR_LATEX: :width 0.75\textwidth
[[./Images/PES.pdf]]
#+END_CENTER

*** Gradients

*** Hessians

*** Minima

**** Local

**** Global

*** Saddle points

**** First-order

**** Higher order

*** Minimum energy paths

** Energy gradients and second derivatives

** Optimization algorithms
*** Steepest descent

*** Conjugate gradient

*** Quasi-Newton Raphson


*** Direct inversion in the iterative subspace (DIIS)
*** Trudge
*** Genetic algorithm
*** Convergence criteria

** Geometry optimization algorithms

** Efficient coordinate systems
** Performance of models
** Vibrational frequencies
** Transition states
** Intrinsic reaction coordinates
\newpage

** Molecular dynamics

* =GAMESS= Cheat Sheet
** Specifying electronic configuration
*** Closed shell (default)
#+BEGIN_EXAMPLE
 $CONTRL RHFTYP=RHF MULT=1 $END
#+END_EXAMPLE

*** Open-shell (n = spin-multiplicity = # unpaired electrons + 1)
#+BEGIN_EXAMPLE
 $CONTRL RHFTYP=UHF MULT=n $END
#+END_EXAMPLE
** Specifying electronic structure method
*** Hartree-Fock (default)
*** DFT
#+BEGIN_EXAMPLE
 $CONTRL DFTTYP=xxx $END
#+END_EXAMPLE
=Gamess= supports _many_ DFT functionals.  See the $DFT section of the manual for a full set.  Common DFT methods include:
|-----------------------------+--------|
| method                      | xxx    |
|-----------------------------+--------|
| Slater                      | Slater |
| Local density approx        | SVN    |
| Generalized gradient approx | PBE    |
| Hybrid DFT                  | B3LYP  |
| "Minnesota" optimized       | M06    |
|                             | M11    |
|-----------------------------+--------|
*** Beyond Hartree-Fock
Many methods available.  See manual for full description.  Most common is second-order perturbation theory, "MP2," :
#+BEGIN_EXAMPLE
 $CONTRL MPLEVL=2 $END
#+END_EXAMPLE

If you want a very high quality number, have a big computer and time to wait, try "coupled cluster,":
#+BEGIN_EXAMPLE
 $CONTRL CCTYP=CCSD(T) $END
#+END_EXAMPLE

** Specifying basis sets
Gamess uses atom-centered basis functions.  A "basis set" is a set of such functions for many atoms, all constructed (hopefully) at a consistent level of accuracy. Many such basis sets exist and are coded into Gamess, and for the daring new basis sets can be input by hand.  Choice of basis set is always a compromise between accuracy and computational cost.  In general should always check sensitivity of property of interest to basis set.

Specified in $BASIS group.  Some common choices, in increasing level of sophistication:

|--------------+--------------------------------+----------------------------------------|
| Name         | Type                           | Flags                                  |
|--------------+--------------------------------+----------------------------------------|
|              | *Pople type*                   | *The most venerable and widely used*   |
| STO-3G       | Minimal                        | GBASIS=STO  NGAUSS = 3                 |
| 3-21G        | Split valence                  | GBASIS=N21  NGAUSS=3                   |
| 6-31G(d)     | Split valence polarized        | GBASIS=N31 NGAUSS =6 NDFUNC=1          |
| 6-311+G(d,p) | Triple-split valence           | GBASIS=N311 NGAUSS=6 NDFUNC=1 NPFUNC=1 |
|              | polarized and augmented        |      DIFFSP=1                          |
|              |                                |                                        |
|              | *Polarization-consistent*      | *Good for DFT*                         |
| PC0          | Minimal                        | GBASIS=PC0                             |
| PC1          | Split valence polarized        | GBASIS=PC1                             |
| PC2          | Triple split double polarized  | GBASIS=PC2                             |
|              |                                |                                        |
|              | *Correlation-consistent*       | *Good for MP2 and beyond*              |
| cc-pVDZ      | Split valence polarized        | GBASIS=CC2                             |
| cc-pVTZ      | Triple split double polarized  | GBASIS=CC3                             |
|              |                                |                                        |
|              | *Effective core potentials*    | *Good for treating heavy atoms*        |
| SBKJC        | Split valence + core potential | GBASIS=SBKJC                           |
| Hay-Wadt     | Split valence + core potential | GBASIS=HW                              |
|--------------+--------------------------------+----------------------------------------|

** Specifying geometry
Again =GAMESS= has a number of options, several of which are arcane and seldom used.  Most common are Cartesian and z-matrix.  Here I give examples ignoring any symmetry the molecule might have.
*** Cartesian
Specify an atom name, atomic number, and cartesian positions in \AA.  Over-specified, so code will typically center and reorient.  Following is for gauche difluoroethane, \ce{FCH2CH2F}.
#+BEGIN_EXAMPLE
 $CONTRL COORD=CART $END
 $DATA
FCH2CH2F drag calculation
C1
C     6.0    -3.76764     0.33879     0.03727
C     6.0    -2.35246     0.34495     0.03689
F     9.0    -4.72277     0.58147    -1.18012
F     9.0    -1.59909    -0.68487    -0.83662
H     1.0    -4.04387     1.08375     0.75395
H     1.0    -3.92958    -0.71060     0.16941
H     1.0    -2.03786     0.18875     1.04760
H     1.0    -2.09983     1.28759    -0.40187
 $END
#+END_EXAMPLE

*** Z-matrix
Specify atom name, number of atom it is connected to, distance to that atom, number of atom it makes an angle with, value of the angle, number of the atom it makes a dihedral with, and value of the dihedral angle.  Values may be given directly or as variables, followed by list of variable specifications.  In =Gamess=, would specify z-matrix for \ce{FCH2CH2F} like this:
#+BEGIN_EXAMPLE
$CONTRL COORD=ZMT $END
$DATA
FCH2CH2F drag calculation
C1
C
C   1   r1
F   2   r2   1   A1
H   2   r3   1   A2   3   D1
H   2   r4   1   A3   3   D2
F   1   r2   2   A1   3   D3
H   1   r3   2   A2   6   D1
H   1   r4   2   A3   6   D2

r1=1.5386
r2=1.39462
r3=1.11456
r4=1.12
A1=109.54214
A2=111.
A3=110.
D1=120.
D2=-120.5
D3=50.
$END
#+END_EXAMPLE
Particularly convenient when you'd like to "scan" over the value of some coordinate.  Variable can be applied to more than one independent coordinate, if the molecule has symmetry.  In general, though, variables should not be reused.

** Specify calculation type
Specified in $CONTRL group by RUNTYP flag:

| Calculation                   | RUNTYP=  |
|-------------------------------+----------|
| Single-point energy           | Energy   |
| Single-point energy + force   | Gradient |
| Geometry optimization         | Optimize |
| Frequency calculation         | Hessian  |
| Transition state search       | Sadpoint |
| Intrinisc reaction coordinate | IRC      |

Note too that specifying EXETYP=CHECK will check your input without actually running the job.
** Geometry optimization coordinate system
By default, =GAMESS= performs optimizations in Cartesian coordinates.

To use z-matrix coordinates, specify
#+BEGIN_EXAMPLE
 $CONTRL COORDS=ZMT NZVAR=xx $END
#+END_EXAMPLE
where NZVAR = 3N-6.

To have =Gamess= automatically create a set of appropriate "delocalized" internal coordinates, specify
#+BEGIN_EXAMPLE
 $ZMAT DLC=.TRUE.  AUTO=.TRUE. $END
#+END_EXAMPLE
** Performing a transition state search
Transition states are always more challenging to find than optimal geometries.  Always ~at least~ a three-step procedure:

1. Guess a transition state structure and compute Hessian matrix (RUNTYP = HESSIAN).  Check to be sure there is one imaginary mode approximating desired TS.
2. Perform the transition state search.  Must supply computed Hessian matrix, copied from .dat file, as $HESS group.  Also should specify $STATPT HESS=READ $END.
3. Check the result!  Assuming the transition state search converges, run another frequency calculation to confirm that there is one and only one imaginary mode, and that it corresponds to desired TS.

* Plane waves and core potentials

** Hydrogen atom in a box

** Periodic boundary conditions

** Supercells - Cartesian and fractional coordinates

** Gaussian vs. Vasp

** Vasp POSCAR

** Vasp INCAR

** Core electron treatment

*** OPW

*** PP

*** PAW

** Comparing energies between calculations

** Wavefunctions and charge densities

** Exploring potential energy surfaces
\newpage
* Periodic electronic structure
** Isolated vs. periodic systems
** Bloch's theorem and qualitative band structure
** Band folding
** Multi-dimensional periodicity
** Density of statues
** Bravais lattices
** Quantitative supercell calculations
** Brillouin zone integration
*** k-point sampling
*** Fermi smearing
\newpage
* Practical supercell calculations
\newpage
* Surfaces
** Surface planes
** Slab models
** Surface energy
** Surface potentials and Fermi energies
** Surface adsorption
** Coverage-dependent adsorption
** Reaction barriers

\newpage
* First-principles thermodynamics
** Connection Between QM and Thermodynamics
We have focused to this point on the many approaches and details of calculating
the /internal electronic energy/ of a single molecule, that is, the energy
associated with taking infinitely separated constituent nuclei and electrons at
rest and forming a molecule:
\begin{equation}
2~\mathrm{H}^+ + 8~\mathrm{O}^{8+} + 10~\mathrm{e}^- \rightarrow
\mathrm{H_2O}\qquad E^\mathrm{elec}
\end{equation}
$E^\mathrm{elec}$ is typically calculated within the Born-Oppenheimer
approximation, i.e.\ within the approximation that the nuclei are fixed in space
at the minimum energy configuration.  Even at 0~K, by quantum mechanics the
atoms must vibrate about this minimum, and this intrinsic vibration imparts a
/zero-point vibrational energy/ (ZPVE) to the molecule, and the 0~K
internal energy of a molecule is thus:
\begin{equation}
  E^0=E^\mathrm{elec} + ZPVE
\end{equation}
ZPVE can be calculated reliably within the harmonic approximation, according to
\begin{equation}
  \mathrm{ZPVE}=\frac{1}{2}h\sum_{i=1}^{3n-6}\nu_i
\end{equation}
where $\nu_i$ are the harmonic vibrational frequencies, obtained from a
vibrational frequency analysis.  $E^0$ is the minimum physically meaninfful
energy of the molecule.

Energy can be deposited in a molecule in many other ways as well, e.g.\ as
translational and rotational kinetic energy, in excited vibrational modes, in
the interaction of a molecule with an external electric or magnetic or
gravitational field, or ....  If we assume that the energy in these various
degrees of freedom are separable, we can write:
\begin{equation}
  E_i=E^0+E^\mathrm{trans}+E^\mathrm{rot}+E^\mathrm{vib} +E^\mathrm{elec*}+E^\mathrm{ext}
\end{equation}
To fully describe microscopic energetic state of a molecule, would have to
specify all of these.

Typically, though, we are more interested in the collective properties of many
molecules at equilibrium, like the internal energy $U$ or enthalpy $H$ or Gibbs
energy $G$, under some external constraints like temperature $T$ or volu
me $V$.  These thermodynamic quantities are averages over the energy states of
an /ensemble/ of molecules.  The way this averaging is performed is the
realm of /statistical thermodynamics/.

Most important for us will be the /canonical ensemble/, in which the free
variables are the number of molecules $N$, the total volume $V$, and the
temperature $T$.  Offer without proof, in the canonical ensemble the
probability for a molecule to be in some energy state $E_i$ above $E^0$ is
given by the Boltzmann factor,
\begin{equation}
  P(E_i) \propto e^{-E_i\beta}=e^{-E_i/k_BT},\qquad\beta=1/k_BT
\end{equation}
Defines an exponentially decaying probability function for a state to be
occupied at some temperature.  In a sense, /temperature/ is the property of
a system following this distribution.
\begin{figure}[h]
  \centering
  \includegraphics{./Images/boltzmann}
  \caption{Boltzmann distribution at two different temperatures}
  \label{fig:boltzmann}
\end{figure}

*** Averages and partition functions
Let's use this to calculate the internal energy $U$ of a molecule at some
temperature.
\begin{equation}
  U(T)=\frac{\sum_iE_iP(E_i)}{\sum_iP(E_i)}
\end{equation}
where the denominator ensures that the probability is normalized.
\begin{eqnarray}
  U(T) & =& \frac{\sum_iE_i e^{-E_i\beta}}{\sum_ie^{-E_i\beta}} \\
  & = & \frac{\frac{\partial}{\partial\beta}\sum_ie^{-E_i\beta}}{\sum_ie^{-E_i
      \beta}}\\
& = & -\frac{\partial \ln \sum_i e^{-E_i\beta}}{\partial \beta}
\end{eqnarray}
The sum over energy states is evidently a special quantity, called the
partition function:
\begin{equation}
  q=\sum_ie^{-E_i\beta}
\end{equation}
All thermodynamic quantities can be written in terms of the partition function!

*** Harmonic oscillator example
Harmonic oscillator is a reasonable model of a molecular vibration.  Energy
spectrum given by
\begin{equation}
  E_v=(v+1/2)h\nu,\qquad v=0,1,2,...
\end{equation}
Let's define the energy qunatum $h\nu=\epsilon_0$ and reset the energy scale so
that zero is at $1/2 h\nu$:
\begin{eqnarray}
  E_v & = & v\epsilon_0,\qquad v=0,1,2,... \\
q(T) & = &\sum_{v=0}^\infty e^{-v\epsilon_0\beta} \\
 & = & \frac{1}{1-e^{-\epsilon_0\beta}}
\end{eqnarray}
where we take advantage of the fact that the sum is a geometric series to
evaluate it in closed form.

Plot partition function vs $T$, increasing function.

\noindent Internal energy:
\begin{eqnarray}
  U(T) &=&-\frac{\partial \ln q}{\partial \beta}\\
   & = & \frac{\epsilon_0}{e^{\epsilon_0\beta}-1}
\end{eqnarray}

\noindent Heat capacity:

\noindent Entropy

** Molecular Ideal Gas
Nice example above for a simple model.  To get thermodynamics of an ideal gas,
in principle need to sum over all the types of energy states (translational,
rotational, vibrational, ...) of every molecule.  Seemingly impossible task.
One simplification is if we can write energy as sum of energies of individual
elements (molecules) of system:
    \begin{align}
      E_j&=\epsilon_j(1)+\epsilon_j(2) + ... + \epsilon_j(N) \\
      Q(N,V,T) &= \sum_j e^{-E_j\beta} \\
      &=\sum_je^{-(\epsilon_j(1)+\epsilon_j(2) + ... + \epsilon_j(N))\beta}
    \end{align}
/If/ molecules/elements of system can be distinguished from each
        other (like atoms in a fixed lattice), expression can be factored:
      \begin{align}
        Q(N,V,T)&=\left ( \sum_j e^{-\epsilon_j(1)\beta}\right )\cdots \left ( \sum_j
          e^{-\epsilon_j(N)\beta}\right ) \\
      &= q(1)\cdots q(N) \\
      \text{Assuming all the elements are the same:}\\
      &= q^N
    \end{align}
/If not/ distinguishable (like molecules in a liquid or gas, or
      electrons in a solid), problem is difficult, because identical
      arrangements of energy amongst elements should only be counted once.
      Approximate solution, good almost all the time:
    \begin{equation}
      Q(N,V,T)=q^N/N!
    \end{equation}
 Sidebar: ``Correct'' factoring depends on whether individual elements
      are fermions or bosons, leads to funny things like superconductivity and
      superfluidity.

This $q(V,T)$ is the /molecular partition function/, and is calculated by
summing over the individual energy states of a single molecule (starting at $E_0$).

Further simplified by factoring into contributions from various ($3N$) molecular
degrees of freedom:
\begin{eqnarray}
  q(V,T)&=&\left(\sum_\mathrm{trans}
    e^{-e_\mathrm{trans}\beta}\right) \left(\sum_\mathrm{rot}
  e^{-e_\mathrm{rot}\beta}\right) \left( \sum_\mathrm{vib}
  e^{-e_\mathrm{vib}\beta} \right) \left( \sum_\mathrm{elec}
  e^{-e_\mathrm{elec}\beta}\right) \\
&=& q_\mathrm{trans}q_\mathrm{rot}q_\mathrm{vib}q_\mathrm{elec} \\
U & = & E_0 + U_\mathrm{trans}+U_\mathrm{rot}+U_\mathrm{vib}+U_\mathrm{elec}
\end{eqnarray}
Similarly for other thermodynamic quantities, for example,
\begin{equation}
  C_v=\left(\frac{\partial U}{\partial T}\right)_V = C_{v,\mathrm{trans}}+C_{v,\mathrm{rot}}+C_{v,\mathrm{vib}}+C_{v,\mathrm{elec}}
\end{equation}
Thermodynamic quantities are sums of contributions from indvidual degrees of
freedom.

Have to somehow /model/ these motions and have to use our quantum
mechanical results to parameterize the models.

*** Translational partition function
Need a model molecules freely translating about in a box.  How about the
/particle in a box/?
\begin{equation}
  E_n=\frac{n^2\pi^2\hbar^2}{2 m L^2},~~~~~n=1,2,3,...
\end{equation}
Have to construct partition function for one molecule.  For gas molecules at
normal conditions, energy spacing is tiny.  Spare the details, but find that
$q_\mathrm{trans}$ can be written in terms of a /thermal wavelength/
$\Lambda$:
\begin{eqnarray}
  \Lambda=\frac{h}{\sqrt{2\pi m k_B T}} \\
q_\mathrm{trans}=\frac{V^\circ}{\Lambda^3}
\end{eqnarray}
$\Lambda$ depends only a molecule mass (that's easy!) and is of the order the
box dimensions at which quantization is evident.  Typically a tiny number
(e.g. $1.7\times 10^{-11}$~m for Ar in a 1 liter volume at 298 K.
$q_\mathrm{trans}$ is, on the other hand, enormous: lots of translational
freedom.  $V^\circ$ defines the standard state volume.

Given this, can find all translational contributions to thermodynamics.
$S_\mathrm{trans}$ gives the Sackur-Tetrode equation, the absolute entropy of a
monatomic gas:


*** Rotational partition function
Model molecule as a rigidly rotating body.  Body has three orthogonal moments
of inertia $I$ determined by the molecular structure.

*** Vibrational partition function
See harmonic oscillator above

*** Electronic partition function
Governed by Fermi-Dirac distribution
Electronic degeneracy


** CO T-dependent thermo example
\newpage
* Implicit solvation
* Density functional theory
** Electron density \rho as fundamental quantity
** Thomas-Fermi-Dirac model
** Hartree-Fock-Slater model
** Hohenberg-Kohn theorems
** Kohn-Sham construction
** Exchange-correlation functionals
So Kohn et al. showed that the DFT approach is theoretically well-grounded and
provided one way to practically apply it. Promise is that if we can find an
approximation to the (unknown) true \(v_\text{xc}\) with the right balance of simplicity
and accuracy, we will have one sweet theory. Has to incorporate both exchange,
like Slater tried to do, and correlation.

How to proceed? Lots of approaches, and jargon here is at least as bad as in
wavefunction-based methods. Perdew 2006 describes the ``Jacob's ladder'' of
approximations:
*** LDA
One well-defined limit is the homogeneous electron gas, and this is the usual
starting point for modern approximate DFT methods. Assume exchange and
correlation potentials at any given point depend only on the value of \rho there
(or spin-up and spin-down \rho, if spin-polarized). We know from Slater and
Dirac's work what the exchange potential is for this system.

It is possible to determine numerically the correlation energy for a given
density from quantum Monte Carlo calculations. Ceperley and Alder (PRL 1980,
45, 566) did this to very high accuracy, and others (Vosko, Wilk, and Nusair,
"VWN", and Perdew and Wang, "PW") fit these numerical results to analytical
models in \rho. This combination of local exchange and correlation defines the LDA
model.

LDA offers modest improvement over HFS for molecules. ``Homogeneous''
approximation pretty severe for an atom or molecule. Nonetheless, works
surprisingly well for structures and charge distributions, but has problems in
calculating accurate bond energies, typically overbinding. Also tends to
underestimate the HOMO-LUMO gap in molecules and analogous band gap in solids.

*** GGA

*** Meta-GGA

*** Hyper GGA and hybrid functionals

**** ``Screened'' exchange

*** Beyond hyper GGA

** Implementations

** Performance
\newpage
\newpage
* Electron correlation methods
\newpage
